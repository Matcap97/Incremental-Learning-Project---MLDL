{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "svm exemplars selections.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQwww4igA07y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6c36a639-bf28-424b-cf49-6d34807c3582"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision.datasets import VisionDataset, CIFAR100\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import ChainMap\n",
        "\n",
        "\n",
        "np.random.seed(1993) \n",
        "order = np.arange(100)\n",
        "np.random.shuffle(order)\n",
        "print(order)\n",
        "label_map = {k: v for v, k in enumerate(order)}\n",
        "\n",
        "\n",
        "lala=[order[10*x:10*(x+1)].tolist() for x in range(0, 10)]\n",
        "\n",
        "print(lala)\n",
        "print(label_map)\n",
        "label_map_special = {}\n",
        "\n",
        "for i in range(10):\n",
        "  for j, w in enumerate(lala[i]):\n",
        "    if w not in label_map_special:\n",
        "      label_map_special[w] = j\n",
        "\n",
        "assert 100 == len(list(label_map_special.keys()))\n",
        "\n",
        "print(label_map_special)\n",
        "\n",
        "\n",
        "\n",
        "train = CIFAR100('.data/', train=True, transform=None, download=True)\n",
        "test = CIFAR100('.data/', train=False, transform=None, download=True)\n",
        "\n",
        "class Cifar100:\n",
        "    def __init__(self, train, test):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.train_groups, self.test_groups = self.split()\n",
        "        self.batch_num = 10\n",
        "\n",
        "    def split(self):\n",
        "        train_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.train:\n",
        "            if target in order[:10]:\n",
        "                train_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                train_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                train_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                train_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                train_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                train_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                train_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                train_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                train_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                train_groups[9].append((data,target))\n",
        "        assert len(train_groups[0]) == 5000, len(train_groups[0])\n",
        "        assert len(train_groups[1]) == 5000, len(train_groups[1])\n",
        "        assert len(train_groups[2]) == 5000, len(train_groups[2])\n",
        "        assert len(train_groups[3]) == 5000, len(train_groups[3])\n",
        "        assert len(train_groups[4]) == 5000, len(train_groups[4])\n",
        "        assert len(train_groups[5]) == 5000, len(train_groups[5])\n",
        "        assert len(train_groups[6]) == 5000, len(train_groups[6])\n",
        "        assert len(train_groups[7]) == 5000, len(train_groups[7])\n",
        "        assert len(train_groups[8]) == 5000, len(train_groups[8])\n",
        "        assert len(train_groups[9]) == 5000, len(train_groups[9])\n",
        "\n",
        "\n",
        "\n",
        "        test_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.test:\n",
        "            if target in order[:10]:\n",
        "                test_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                test_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                test_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                test_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                test_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                test_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                test_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                test_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                test_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                test_groups[9].append((data,target))\n",
        "        assert len(test_groups[0]) == 1000\n",
        "        assert len(test_groups[1]) == 1000\n",
        "        assert len(test_groups[2]) == 1000\n",
        "        assert len(test_groups[3]) == 1000\n",
        "        assert len(test_groups[4]) == 1000\n",
        "        assert len(test_groups[5]) == 1000\n",
        "        assert len(test_groups[6]) == 1000\n",
        "        assert len(test_groups[7]) == 1000\n",
        "        assert len(test_groups[8]) == 1000\n",
        "        assert len(test_groups[9]) == 1000\n",
        "\n",
        "        return train_groups, test_groups\n",
        "\n",
        "    def next_classes_batch(self, i):\n",
        "        return self.train_groups[i], self.test_groups[i]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cifar = Cifar100(train, test)\n",
        "    print(len(cifar.train_groups[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[68 56 78  8 23 84 90 65 74 76 40 89  3 92 55  9 26 80 43 38 58 70 77  1\n",
            " 85 19 17 50 28 53 13 81 45 82  6 59 83 16 15 44 91 41 72 60 79 52 20 10\n",
            " 31 54 37 95 14 71 96 98 97  2 64 66 42 22 35 86 24 34 87 21 99  0 88 27\n",
            " 18 94 11 12 47 25 30 46 62 69 36 61  7 63 75  5 32  4 51 48 73 93 39 67\n",
            " 29 49 57 33]\n",
            "[[68, 56, 78, 8, 23, 84, 90, 65, 74, 76], [40, 89, 3, 92, 55, 9, 26, 80, 43, 38], [58, 70, 77, 1, 85, 19, 17, 50, 28, 53], [13, 81, 45, 82, 6, 59, 83, 16, 15, 44], [91, 41, 72, 60, 79, 52, 20, 10, 31, 54], [37, 95, 14, 71, 96, 98, 97, 2, 64, 66], [42, 22, 35, 86, 24, 34, 87, 21, 99, 0], [88, 27, 18, 94, 11, 12, 47, 25, 30, 46], [62, 69, 36, 61, 7, 63, 75, 5, 32, 4], [51, 48, 73, 93, 39, 67, 29, 49, 57, 33]]\n",
            "{68: 0, 56: 1, 78: 2, 8: 3, 23: 4, 84: 5, 90: 6, 65: 7, 74: 8, 76: 9, 40: 10, 89: 11, 3: 12, 92: 13, 55: 14, 9: 15, 26: 16, 80: 17, 43: 18, 38: 19, 58: 20, 70: 21, 77: 22, 1: 23, 85: 24, 19: 25, 17: 26, 50: 27, 28: 28, 53: 29, 13: 30, 81: 31, 45: 32, 82: 33, 6: 34, 59: 35, 83: 36, 16: 37, 15: 38, 44: 39, 91: 40, 41: 41, 72: 42, 60: 43, 79: 44, 52: 45, 20: 46, 10: 47, 31: 48, 54: 49, 37: 50, 95: 51, 14: 52, 71: 53, 96: 54, 98: 55, 97: 56, 2: 57, 64: 58, 66: 59, 42: 60, 22: 61, 35: 62, 86: 63, 24: 64, 34: 65, 87: 66, 21: 67, 99: 68, 0: 69, 88: 70, 27: 71, 18: 72, 94: 73, 11: 74, 12: 75, 47: 76, 25: 77, 30: 78, 46: 79, 62: 80, 69: 81, 36: 82, 61: 83, 7: 84, 63: 85, 75: 86, 5: 87, 32: 88, 4: 89, 51: 90, 48: 91, 73: 92, 93: 93, 39: 94, 67: 95, 29: 96, 49: 97, 57: 98, 33: 99}\n",
            "{68: 0, 56: 1, 78: 2, 8: 3, 23: 4, 84: 5, 90: 6, 65: 7, 74: 8, 76: 9, 40: 0, 89: 1, 3: 2, 92: 3, 55: 4, 9: 5, 26: 6, 80: 7, 43: 8, 38: 9, 58: 0, 70: 1, 77: 2, 1: 3, 85: 4, 19: 5, 17: 6, 50: 7, 28: 8, 53: 9, 13: 0, 81: 1, 45: 2, 82: 3, 6: 4, 59: 5, 83: 6, 16: 7, 15: 8, 44: 9, 91: 0, 41: 1, 72: 2, 60: 3, 79: 4, 52: 5, 20: 6, 10: 7, 31: 8, 54: 9, 37: 0, 95: 1, 14: 2, 71: 3, 96: 4, 98: 5, 97: 6, 2: 7, 64: 8, 66: 9, 42: 0, 22: 1, 35: 2, 86: 3, 24: 4, 34: 5, 87: 6, 21: 7, 99: 8, 0: 9, 88: 0, 27: 1, 18: 2, 94: 3, 11: 4, 12: 5, 47: 6, 25: 7, 30: 8, 46: 9, 62: 0, 69: 1, 36: 2, 61: 3, 7: 4, 63: 5, 75: 6, 5: 7, 32: 8, 4: 9, 51: 0, 48: 1, 73: 2, 93: 3, 39: 4, 67: 5, 29: 6, 49: 7, 57: 8, 33: 9}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6PHZtTr8BNay",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import skimage.io as io\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "class BatchData(Dataset):\n",
        "    def __init__(self, images, labels, special_map=None, input_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.input_transform = input_transform\n",
        "        self.special_map = special_map\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        image = Image.fromarray(np.uint8(image))\n",
        "        label = self.labels[index]\n",
        "        if self.special_map is not None:\n",
        "          label = label_map_special[label]\n",
        "        else:  \n",
        "          label = label_map[label]\n",
        "        if self.input_transform is not None:\n",
        "          image = self.input_transform(image)\n",
        "        \n",
        "        label = torch.LongTensor([label])\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jl_Ow_k1R0sT",
        "colab": {}
      },
      "source": [
        "class Exemplar:\n",
        "    def __init__(self, max_size, total_cls):\n",
        "        self.train = {}\n",
        "        self.cur_cls = 0\n",
        "        self.max_size = max_size\n",
        "        self.total_classes = total_cls\n",
        "        self.store_num = {}\n",
        "        self.count_train = {}\n",
        "\n",
        "    def update(self, cls_num, train, inc_i):\n",
        "        train_x, train_y = train\n",
        "        cur_keys = list(set(train_y))\n",
        "        print(cur_keys)\n",
        "        def countX(tup, x): \n",
        "            count = 0\n",
        "            for ele in tup: \n",
        "              if (ele == x): \n",
        "                count = count + 1\n",
        "            return count \n",
        "        \n",
        "        self.cur_cls = cls_num\n",
        "\n",
        "        for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
        "            self.count_train[i] = countX(train_y, i)     \n",
        "            self.store_num[i] = int(self.count_train[i] / ((self.cur_cls)*0.1))\n",
        "\n",
        "        \n",
        "        total_store_num = self.max_size / self.cur_cls #if self.cur_cls != 0 else max_size\n",
        "        train_store_num = int(total_store_num)  # ha senso?\n",
        "\n",
        "        for x, y in zip(train_x, train_y):\n",
        "            if y not in self.train:\n",
        "                self.train[y] = [x]\n",
        "            else:\n",
        "                #if len(self.train[y]) < self.store_num[y]:\n",
        "                if len(self.train[y]) < train_store_num:\n",
        "                    self.train[y].append(x)\n",
        "        assert self.cur_cls == len(list(self.train.keys()))\n",
        "        \n",
        "        for key, value in self.train.items():\n",
        "            #self.store_num[key] = int(self.count_train[key] / ((self.cur_cls)*0.1))\n",
        "            #self.train[key] = value[:self.store_num[key]]\n",
        "            self.train[key] = value[:train_store_num]\n",
        "        for key, value in self.train.items():\n",
        "            #assert len(self.train[key]) == self.store_num[key]\n",
        "            assert len(self.train[key]) <= train_store_num\n",
        "\n",
        "\n",
        "    def get_exemplar_train(self):\n",
        "        exemplar_train_x = []\n",
        "        exemplar_train_y = []\n",
        "        for key, value in self.train.items():\n",
        "            for train_x in value:\n",
        "                exemplar_train_x.append(train_x)\n",
        "                exemplar_train_y.append(key)\n",
        "        return exemplar_train_x, exemplar_train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w5QraSvYBTIV",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lx2V_-OIBg0e",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        self.old_model = None\n",
        "        self.num_classes = num_classes\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc = nn.Linear(64 * block.expansion, self.num_classes)\n",
        "  \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, features=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        if features:\n",
        "            x = x / x.norm()\n",
        "        else:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Hokz7LkBqnv",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Scale, Resize, ToTensor, ToPILImage\n",
        "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss    \n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import dml\n",
        "from dml.ncmc import NCMC_Classifier\n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import PIL.Image as Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "\n",
        "def weight_reset(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, total_cls):\n",
        "        self.total_cls = total_cls\n",
        "        self.seen_cls = 0\n",
        "        self.dataset = Cifar100(train, test)\n",
        "        self.model = resnet32(num_classes = total_cls).cuda()\n",
        "\n",
        "        self.means = {}\n",
        "        transform_train = [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "        ]\n",
        "        transform_train.extend([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "        self.input_transform = Compose(transform_train)\n",
        "        \n",
        "        self.input_transform_eval = Compose([\n",
        "                                ToTensor(),\n",
        "                                Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010))])\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"Solver total trainable parameters : \", total_params)\n",
        "\n",
        "\n",
        "    def test(self, testdata):\n",
        "        # print(\"test data number : \",len(testdata))\n",
        "        self.model.eval()\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "        with torch.no_grad():\n",
        "          for i, (image, label) in enumerate(testdata):\n",
        "              image = image.cuda()\n",
        "              label = label.view(-1).cuda()\n",
        "              p = self.model(image)\n",
        "              pred = p[:,:self.seen_cls].argmax(dim=-1)\n",
        "              correct += sum(pred == label).item()\n",
        "              wrong += sum(pred != label).item()\n",
        "        acc = correct / (wrong + correct)\n",
        "        print(f\"\\r Test Acc: {acc*100} \\n\")\n",
        "        return acc\n",
        "\n",
        "    def NMEClassifier(self, test_loader, train_loader):\n",
        "      self.model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for i in range(self.seen_cls-10,self.seen_cls):\n",
        "          t=0\n",
        "          mean = torch.zeros((1,64),device='cuda')\n",
        "          for indices,(images,labels) in enumerate((train_loader)):\n",
        "            images = images.to('cuda')\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,key in zip(outputs,labels):\n",
        "              if i==key:\n",
        "                mean+=output\n",
        "                t+=1\n",
        "          mean = mean/t\n",
        "          self.means[i] = mean / mean.norm()\n",
        "        correct = 0.0\n",
        "\n",
        "        for _, (images, labels) in enumerate((test_loader)):\n",
        "          images = images.to('cuda')\n",
        "          outputs = self.model(images,features=True)\n",
        "          preds = []\n",
        "          for output in outputs:\n",
        "            pred = None\n",
        "            min_dist = 10000000\n",
        "            for key in self.means:\n",
        "              dist = torch.dist(self.means[key],output)\n",
        "              if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                pred = key\n",
        "            preds.append(pred)\n",
        "      \n",
        "          for label, pred in zip(labels,preds):\n",
        "            if label == pred:\n",
        "              correct += 1\n",
        "        acc = correct/(self.seen_cls*100)\n",
        "        print(f'NME Accuracy: {acc} \\n*****************\\n')\n",
        "      return acc\n",
        "\n",
        "    def SVMClassifier(self,test_loader,train_loader,centroids):\n",
        "      self.model.eval()\n",
        "      x_train=[]\n",
        "      y_train=[]\n",
        "      x_test=[]\n",
        "      y_test=[]\n",
        "      with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate((train_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              x_train.append(np.array(output.to('cpu')))\n",
        "              y_train.append(np.array(label))\n",
        "        norm = MinMaxScaler() # Normalization for the test phase.\n",
        "\n",
        "        norm.fit(x_train)\n",
        "        x_train = norm.transform(x_train)\n",
        "\n",
        "        n_correct = 0.0\n",
        "        for i, (images, labels) in enumerate((test_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              x_test.append(np.array(output.to('cpu')))\n",
        "              y_test.append(np.array(label))\n",
        "              \n",
        "        x_test = norm.transform(x_test)\n",
        "      svc = SVC(random_state=42, C=1)\n",
        "      svc.fit(x_train, y_train)\n",
        "      y_pred=svc.predict(x_test)\n",
        "      acc1 = accuracy_score(y_test, y_pred)\n",
        "      print(f'accuracy SVC:{acc1}')\n",
        "      return acc1\n",
        "\n",
        "    def get_lr(self, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            return param_group['lr']\n",
        "\n",
        "    def train(self, batch_size, epoches, lr, max_size):\n",
        "        total_cls = self.total_cls\n",
        "        criterion = CrossEntropyLoss()\n",
        "        # criterion = BCEWithLogitsLoss()\n",
        "\n",
        "        previous_model = None\n",
        "        exemplar = Exemplar(max_size, total_cls)\n",
        "        dataset = self.dataset\n",
        "        test_xs = []\n",
        "        test_ys = []\n",
        "        train_xs = []\n",
        "        train_ys = []\n",
        "\n",
        "        test_accs = []\n",
        "        for inc_i in range(dataset.batch_num):\n",
        "            print(50*'---')\n",
        "            print(\" Incremental batch num: \" , inc_i)\n",
        "            train, test = dataset.next_classes_batch(inc_i)\n",
        "            print(len(train), len(test))\n",
        "            train_x, train_y = zip(*train)\n",
        "            test_x, test_y = zip(*test)\n",
        "            test_xs.extend(test_x)\n",
        "            test_ys.extend(test_y)\n",
        " \n",
        "\n",
        "            self.seen_cls = (total_cls//dataset.batch_num)*(inc_i + 1)\n",
        "            print(\"Seen cls number: \", self.seen_cls)\n",
        "\n",
        "            if inc_i > 0:\n",
        "               train_xs, train_ys = exemplar.get_exemplar_train()\n",
        "               ex_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform), batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "            \n",
        "            train_xs.extend(train_x)\n",
        "            train_ys.extend(train_y)                              \n",
        "    \n",
        "\n",
        "            train_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "            test_data = DataLoader(BatchData(test_xs, test_ys, input_transform=self.input_transform_eval),\n",
        "                        batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9,  weight_decay=2e-4)\n",
        "            scheduler = MultiStepLR(optimizer, milestones=[40, 60], gamma=0.2)\n",
        "\n",
        "\n",
        "            test_acc = []\n",
        "            \n",
        "\n",
        "            for epoch in range(epoches):\n",
        "                cur_lr = self.get_lr(optimizer)\n",
        "                print(f\"\\r EPOCH:{epoch}, LR:{cur_lr}\", end='')\n",
        "                if inc_i > 0:\n",
        "                   self.distill(train_data, criterion, optimizer, inc_i)\n",
        "                   scheduler.step()\n",
        "                else:\n",
        "                   self.stage_1(train_data, criterion, optimizer)\n",
        "                   scheduler.step()\n",
        "\n",
        "            self.previous_model = deepcopy(self.model)\n",
        "            if inc_i > 0:\n",
        "              acc1 = self.SVMClassifier(test_data,train_data)\n",
        "              acc = self.NMEClassifier(test_data,train_data)\n",
        "              acc2 = self.NCMC(test_data,train_data,centroids = self.seen_cls)\n",
        "            else:\n",
        "              acc = self.test(test_data)\n",
        "            test_acc.append(acc)\n",
        "            test_accs.append(max(test_acc))\n",
        "            print(test_accs)\n",
        "            \n",
        "            x_ex, y_ex = self.exemplar_selection(train_x, train_y, inc_i)   \n",
        "            exemplar.update(self.seen_cls, (x_ex, y_ex), inc_i)\n",
        "\n",
        "\n",
        "         \n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(test_accs)+1), test_accs, label = 'Test accuracy')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def cod_one_hot(self, x):\n",
        "        batch_size = 128\n",
        "        y_one_hot = torch.zeros(len(x), self.seen_cls).cuda()\n",
        "        y_one_hot = y_one_hot.scatter(1,x.long().view(-1,1).cuda(),1).cuda()\n",
        "        return y_one_hot\n",
        "\n",
        "    def cod_one_hot_special(self, x):\n",
        "        batch_size = 128\n",
        "        y_one_hot = torch.zeros(len(x), 10).cuda()\n",
        "        y_one_hot = y_one_hot.scatter(1,x.long().view(-1,1).cuda(),1).cuda()\n",
        "        return y_one_hot\n",
        "\n",
        "\n",
        "    def stage_1(self, train_data, criterion, optimizer):\n",
        "        self.model.train()\n",
        "        print(f\"\\r Training new classes... \", end='')\n",
        "        losses = []\n",
        "        for i, (image, label) in enumerate((train_data)):\n",
        "            image = image.cuda()\n",
        "            label = label.view(-1).cuda() \n",
        "            # label = self.cod_one_hot(label)\n",
        "            p = self.model(image)\n",
        "            loss = criterion(p[:,:self.seen_cls], label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        print(f\"\\r stage1 loss: {np.mean(losses)} \\n\", end='')\n",
        "\n",
        "    def exemplar_selection(self, train_x, train_y,inc_i):\n",
        "        print(f\"\\r Selcting exemplars... \", end='')\n",
        "        train_data_special = DataLoader(BatchData(train_x, train_y, input_transform=self.input_transform),\n",
        "                        batch_size=128, shuffle=False, drop_last=True)\n",
        "        self.model.eval()\n",
        "        X_train=[]\n",
        "        y_train=[]\n",
        "        with torch.no_grad():\n",
        "          for i, (images, labels) in enumerate((train_data_special)):\n",
        "             images = images.to('cuda')\n",
        "             outputs = self.model(images, features=True)   #doppia normalizzazione?\n",
        "             for output, label in zip(outputs, labels):\n",
        "                X_train.append(np.array(output.to('cpu')))\n",
        "                y_train.append(np.array(label))\n",
        "          norm = MinMaxScaler() # Normalization \n",
        "\n",
        "          norm.fit(X_train)\n",
        "          X_train = norm.transform(X_train)\n",
        "          if inc_i > 0:\n",
        "            svc = SVC(random_state=42, C=0.4)\n",
        "          else:\n",
        "            svc = SVC(random_state=42, C=0.04)\n",
        "          mod = svc.fit(X_train, y_train)\n",
        "          mod.support_vectors_\n",
        "          mod.support_\n",
        "          mod.n_support_\n",
        "          X_exemplars = []\n",
        "          y_exemplars = []\n",
        "          for i in range(sum(mod.n_support_)):\n",
        "            X_exemplars.append(train_x[int(mod.support_[i])])\n",
        "            y_exemplars.append(train_y[int(mod.support_[i])])\n",
        "          print(f'Number support vectors per class:{mod.n_support_}')\n",
        "          #print(f'Index of support vector per class:{mod.support_}')\n",
        "          print(f'Number support vectors:{sum(mod.n_support_)}')\n",
        "        return X_exemplars, y_exemplars\n",
        "\n",
        "    def distill(self, train_data, criterion, optimizer, inc_i):\n",
        "        self.model.train()\n",
        "        print(f\"\\r Training ... \", end='')\n",
        "        distill_losses = []\n",
        "        for i, (image, label) in enumerate((train_data)):\n",
        "            image = image.cuda()\n",
        "            label = label.view(-1).cuda()\n",
        "            # label = self.cod_one_hot(label)\n",
        "            p = self.model(image)\n",
        "            with torch.no_grad():\n",
        "                pre_p = self.previous_model(image)\n",
        "            pre_p = F.softmax(pre_p[:,:self.seen_cls-10], dim=1)\n",
        "            logp = F.log_softmax(p[:,:self.seen_cls-10], dim=1)\n",
        "            dist_loss = -torch.mean(torch.sum(pre_p * logp, dim=1))-10:self.seen_cls]), 1)\n",
        "            loss_class = CrossEntropyLoss()(p[:,:self.seen_cls], label)\n",
        "            loss = loss_class + dist_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            distill_losses.append(loss.item())\n",
        "        print(f\"\\r distill and bce loss: {np.mean(distill_losses)}\\n\",end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HZZHPrpJBujK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8aee6c29-185d-4f49-cc6d-7239658970bf"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "from torch.utils import *\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Incremental Learning BIC')\n",
        "parser.add_argument('--batch_size', default = 128, type = int)\n",
        "parser.add_argument('--epoch', default = 70, type = int)\n",
        "parser.add_argument('--lr', default = 0.2, type = int)\n",
        "parser.add_argument('--max_size', default = 2000, type = int)\n",
        "parser.add_argument('--total_cls', default = 100, type = int)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "trainer = Trainer(args.total_cls)\n",
        "trainer.train(args.batch_size, args.epoch, args.lr, args.max_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solver total trainable parameters :  472756\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  0\n",
            "5000 1000\n",
            "Seen cls number:  10\n",
            " stage1 loss: 2.252383082340925 \n",
            " stage1 loss: 1.7470086751840053 \n",
            " stage1 loss: 1.507839554395431 \n",
            " stage1 loss: 1.380744607020647 \n",
            " stage1 loss: 1.264107046983181 \n",
            " stage1 loss: 1.216196093803797 \n",
            " stage1 loss: 1.127125356441889 \n",
            " stage1 loss: 1.0831555357346168 \n",
            " stage1 loss: 0.9986807428873502 \n",
            " stage1 loss: 0.9263501320129786 \n",
            " stage1 loss: 0.875148491981702 \n",
            " stage1 loss: 0.866979962740189 \n",
            " stage1 loss: 0.7678204346925784 \n",
            " stage1 loss: 0.7175045487208244 \n",
            " stage1 loss: 0.6729083305750138 \n",
            " stage1 loss: 0.6426067780225705 \n",
            " stage1 loss: 0.6245269668407929 \n",
            " stage1 loss: 0.5780696509740292 \n",
            " stage1 loss: 0.5522309641043345 \n",
            " stage1 loss: 0.525439570347468 \n",
            " stage1 loss: 0.4888074245208349 \n",
            " stage1 loss: 0.46032925293995786 \n",
            " stage1 loss: 0.43281947190944964 \n",
            " stage1 loss: 0.42847430400359326 \n",
            " stage1 loss: 0.4141452167278681 \n",
            " stage1 loss: 0.4009205802128865 \n",
            " stage1 loss: 0.4083215418534401 \n",
            " stage1 loss: 0.38444659878046084 \n",
            " stage1 loss: 0.35423854337288785 \n",
            " stage1 loss: 0.3491615977806923 \n",
            " stage1 loss: 0.32469837291118425 \n",
            " stage1 loss: 0.31435569203816927 \n",
            " stage1 loss: 0.27837465168573916 \n",
            " stage1 loss: 0.2978763889807921 \n",
            " stage1 loss: 0.30807872651479185 \n",
            " stage1 loss: 0.30975953890727115 \n",
            " stage1 loss: 0.26279584413919693 \n",
            " stage1 loss: 0.2534188440976999 \n",
            " stage1 loss: 0.23156411601946905 \n",
            " stage1 loss: 0.21711133687924117 \n",
            " stage1 loss: 0.14523160171050292 \n",
            " stage1 loss: 0.10692887648175924 \n",
            " stage1 loss: 0.0822656740171787 \n",
            " stage1 loss: 0.08034288825897071 \n",
            " stage1 loss: 0.07458221052701657 \n",
            " stage1 loss: 0.0696364684173694 \n",
            " stage1 loss: 0.06988250569273265 \n",
            " stage1 loss: 0.06253918260335922 \n",
            " stage1 loss: 0.058584487973115384 \n",
            " stage1 loss: 0.06087266395871456 \n",
            " stage1 loss: 0.05627314918316328 \n",
            " stage1 loss: 0.051827735339219756 \n",
            " stage1 loss: 0.04599017148407606 \n",
            " stage1 loss: 0.04799896965806301 \n",
            " stage1 loss: 0.04702514983140505 \n",
            " stage1 loss: 0.04494466212315437 \n",
            " stage1 loss: 0.04418166244450288 \n",
            " stage1 loss: 0.03880141097574662 \n",
            " stage1 loss: 0.04265659655898045 \n",
            " stage1 loss: 0.042117758630177915 \n",
            " stage1 loss: 0.039894814626910746 \n",
            " stage1 loss: 0.03818417202012661 \n",
            " stage1 loss: 0.03891763434960292 \n",
            " stage1 loss: 0.03359023558023649 \n",
            " stage1 loss: 0.03557082332479648 \n",
            " stage1 loss: 0.03507562994192808 \n",
            " stage1 loss: 0.037939231747236006 \n",
            " stage1 loss: 0.03793560221600227 \n",
            " stage1 loss: 0.03468183857890276 \n",
            " stage1 loss: 0.034086676696554206 \n",
            " Test Acc: 88.2 \n",
            "\n",
            "[0.882]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[187 184 301 239 192 294 274 312 302 215]\n",
            "Number support vectors:2500\n",
            "[65, 68, 8, 74, 76, 78, 84, 23, 56, 90]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  1\n",
            "5000 1000\n",
            "Seen cls number:  20\n",
            " distill and bce loss: 3.118892047140333\n",
            " distill and bce loss: 2.5297059968665794\n",
            " distill and bce loss: 2.371169059364884\n",
            " distill and bce loss: 2.269573692922239\n",
            " distill and bce loss: 2.1453844330928944\n",
            " distill and bce loss: 2.0760017500983343\n",
            " distill and bce loss: 1.9754914001182273\n",
            " distill and bce loss: 1.9785783776530512\n",
            " distill and bce loss: 1.9092042688970212\n",
            " distill and bce loss: 1.86111628346973\n",
            " distill and bce loss: 1.7993815651646368\n",
            " distill and bce loss: 1.7953658766216702\n",
            " distill and bce loss: 1.7691217837510285\n",
            " distill and bce loss: 1.7342716411308006\n",
            " distill and bce loss: 1.6919841545599479\n",
            " distill and bce loss: 1.6356853070082489\n",
            " distill and bce loss: 1.670624026545772\n",
            " distill and bce loss: 1.5767034203917891\n",
            " distill and bce loss: 1.5556808997083593\n",
            " distill and bce loss: 1.55960128042433\n",
            " distill and bce loss: 1.5262639301794547\n",
            " distill and bce loss: 1.4887193044026692\n",
            " distill and bce loss: 1.501960003817523\n",
            " distill and bce loss: 1.4885063038931952\n",
            " distill and bce loss: 1.480979566220884\n",
            " distill and bce loss: 1.4396997248684917\n",
            " distill and bce loss: 1.4244541349234405\n",
            " distill and bce loss: 1.3794266669838517\n",
            " distill and bce loss: 1.3752674924002752\n",
            " distill and bce loss: 1.3578362972648055\n",
            " distill and bce loss: 1.353273751559081\n",
            " distill and bce loss: 1.329849609622249\n",
            " distill and bce loss: 1.2997499461527224\n",
            " distill and bce loss: 1.2933979564242892\n",
            " distill and bce loss: 1.2879783290403861\n",
            " distill and bce loss: 1.2567594757786504\n",
            " distill and bce loss: 1.2429661022292242\n",
            " distill and bce loss: 1.2503855890697904\n",
            " distill and bce loss: 1.2157434423764546\n",
            " distill and bce loss: 1.2001894734523915\n",
            " distill and bce loss: 1.0319181349542406\n",
            " distill and bce loss: 0.9035012148044728\n",
            " distill and bce loss: 0.873930389130557\n",
            " distill and bce loss: 0.849390427271525\n",
            " distill and bce loss: 0.8348642267562725\n",
            " distill and bce loss: 0.8188302174762443\n",
            " distill and bce loss: 0.8241219862743661\n",
            " distill and bce loss: 0.8092987570497725\n",
            " distill and bce loss: 0.8025509792345541\n",
            " distill and bce loss: 0.7967157352853704\n",
            " distill and bce loss: 0.7972731468854127\n",
            " distill and bce loss: 0.7841582353468295\n",
            " distill and bce loss: 0.775817319198891\n",
            " distill and bce loss: 0.7682865758736929\n",
            " distill and bce loss: 0.7547018296188779\n",
            " distill and bce loss: 0.7593113437846855\n",
            " distill and bce loss: 0.7503983312182956\n",
            " distill and bce loss: 0.7379590306017134\n",
            " distill and bce loss: 0.7508394353919559\n",
            " distill and bce loss: 0.7376001521393105\n",
            " distill and bce loss: 0.7252447958345767\n",
            " distill and bce loss: 0.7210018226393947\n",
            " distill and bce loss: 0.7232254522818106\n",
            " distill and bce loss: 0.7267736041987384\n",
            " distill and bce loss: 0.7196182939741347\n",
            " distill and bce loss: 0.7210602495405409\n",
            " distill and bce loss: 0.7207722674917292\n",
            " distill and bce loss: 0.7210715931874735\n",
            " distill and bce loss: 0.7058946159150865\n",
            " distill and bce loss: 0.7160785970864473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.7325\n",
            "NME Accuracy: 0.7415 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[217 150 260 142 294 179 216 253 201 249]\n",
            "Number support vectors:2161\n",
            "[3, 38, 40, 9, 43, 80, 55, 89, 26, 92]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  2\n",
            "5000 1000\n",
            "Seen cls number:  30\n",
            " distill and bce loss: 3.408705464115849\n",
            " distill and bce loss: 2.342086566819085\n",
            " distill and bce loss: 2.182819949256049\n",
            " distill and bce loss: 2.085913090794175\n",
            " distill and bce loss: 1.9398681035748235\n",
            " distill and bce loss: 1.9263401914525915\n",
            " distill and bce loss: 1.8409158057636685\n",
            " distill and bce loss: 1.7802201641930475\n",
            " distill and bce loss: 1.786618517504798\n",
            " distill and bce loss: 1.702949532756099\n",
            " distill and bce loss: 1.689881717717206\n",
            " distill and bce loss: 1.6795676703806277\n",
            " distill and bce loss: 1.653063138326009\n",
            " distill and bce loss: 1.6069715773617779\n",
            " distill and bce loss: 1.584076088887674\n",
            " distill and bce loss: 1.5620644136711404\n",
            " distill and bce loss: 1.5459142283157066\n",
            " distill and bce loss: 1.5072475495161834\n",
            " distill and bce loss: 1.498671363901209\n",
            " distill and bce loss: 1.5242382332130715\n",
            " distill and bce loss: 1.4966802265908983\n",
            " distill and bce loss: 1.502297706074185\n",
            " distill and bce loss: 1.4818142652511597\n",
            " distill and bce loss: 1.4259767642727605\n",
            " distill and bce loss: 1.376830811853762\n",
            " distill and bce loss: 1.4254124164581299\n",
            " distill and bce loss: 1.402449278919785\n",
            " distill and bce loss: 1.393276748833833\n",
            " distill and bce loss: 1.3439255400940224\n",
            " distill and bce loss: 1.3936855020346466\n",
            " distill and bce loss: 1.3841438072699088\n",
            " distill and bce loss: 1.3354768995885495\n",
            " distill and bce loss: 1.3533999301769115\n",
            " distill and bce loss: 1.3106434720533866\n",
            " distill and bce loss: 1.2837874867297985\n",
            " distill and bce loss: 1.2921016547414992\n",
            " distill and bce loss: 1.2993920688275937\n",
            " distill and bce loss: 1.3020424511697557\n",
            " distill and bce loss: 1.2999049292670355\n",
            " distill and bce loss: 1.2815450407840587\n",
            " distill and bce loss: 1.1168033469606329\n",
            " distill and bce loss: 1.0174678177745253\n",
            " distill and bce loss: 0.9692290292845832\n",
            " distill and bce loss: 0.9432986466972916\n",
            " distill and bce loss: 0.9445686803923713\n",
            " distill and bce loss: 0.9309453257807979\n",
            " distill and bce loss: 0.9163096944491068\n",
            " distill and bce loss: 0.9145022332668304\n",
            " distill and bce loss: 0.9105148911476135\n",
            " distill and bce loss: 0.9035670845596878\n",
            " distill and bce loss: 0.8991834046664061\n",
            " distill and bce loss: 0.8972467769075323\n",
            " distill and bce loss: 0.892383043412809\n",
            " distill and bce loss: 0.8967165858657272\n",
            " distill and bce loss: 0.889091882440779\n",
            " distill and bce loss: 0.8720643001574057\n",
            " distill and bce loss: 0.8769303118741071\n",
            " distill and bce loss: 0.8625949345253132\n",
            " distill and bce loss: 0.8698616358968947\n",
            " distill and bce loss: 0.8707264672826838\n",
            " distill and bce loss: 0.862476955961298\n",
            " distill and bce loss: 0.8565324820854046\n",
            " distill and bce loss: 0.8512900736596849\n",
            " distill and bce loss: 0.8562823787883476\n",
            " distill and bce loss: 0.8658675661793461\n",
            " distill and bce loss: 0.8492330158198321\n",
            " distill and bce loss: 0.8416608748612581\n",
            " distill and bce loss: 0.84936683928525\n",
            " distill and bce loss: 0.8539325760470496\n",
            " distill and bce loss: 0.8608695180327804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.6436666666666667\n",
            "NME Accuracy: 0.6976666666666667 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[152 172 196 208 164 211 124 210 200 156]\n",
            "Number support vectors:1793\n",
            "[1, 70, 77, 17, 50, 19, 85, 53, 58, 28]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  3\n",
            "5000 1000\n",
            "Seen cls number:  40\n",
            " distill and bce loss: 3.453489550837764\n",
            " distill and bce loss: 2.3746234443452625\n",
            " distill and bce loss: 2.1938065422905817\n",
            " distill and bce loss: 2.124982844900202\n",
            " distill and bce loss: 2.051320963435703\n",
            " distill and bce loss: 1.9853823516103957\n",
            " distill and bce loss: 1.940045021198414\n",
            " distill and bce loss: 1.8967173408578943\n",
            " distill and bce loss: 1.8661680155330234\n",
            " distill and bce loss: 1.8432096331207841\n",
            " distill and bce loss: 1.8398107709708038\n",
            " distill and bce loss: 1.7712375941099945\n",
            " distill and bce loss: 1.7252598912627608\n",
            " distill and bce loss: 1.7320711921762537\n",
            " distill and bce loss: 1.6845985540637263\n",
            " distill and bce loss: 1.7050027781062655\n",
            " distill and bce loss: 1.6614014793325353\n",
            " distill and bce loss: 1.6417057072674786\n",
            " distill and bce loss: 1.620043143078133\n",
            " distill and bce loss: 1.59016513382947\n",
            " distill and bce loss: 1.6348007520039876\n",
            " distill and bce loss: 1.6108336625275788\n",
            " distill and bce loss: 1.5509135104991771\n",
            " distill and bce loss: 1.5918251938290067\n",
            " distill and bce loss: 1.5727234946356878\n",
            " distill and bce loss: 1.5539196332295735\n",
            " distill and bce loss: 1.5049404965506659\n",
            " distill and bce loss: 1.4670014602166634\n",
            " distill and bce loss: 1.494012137254079\n",
            " distill and bce loss: 1.4877761558250144\n",
            " distill and bce loss: 1.4588929745886061\n",
            " distill and bce loss: 1.4534732677318432\n",
            " distill and bce loss: 1.4703005331533927\n",
            " distill and bce loss: 1.49285559742539\n",
            " distill and bce loss: 1.4733046889305115\n",
            " distill and bce loss: 1.4422217143906488\n",
            " distill and bce loss: 1.4400443302260504\n",
            " distill and bce loss: 1.4329192152729742\n",
            " distill and bce loss: 1.4112891731438812\n",
            " distill and bce loss: 1.394548561837938\n",
            " distill and bce loss: 1.2417726715405781\n",
            " distill and bce loss: 1.115346974796719\n",
            " distill and bce loss: 1.0960763306529433\n",
            " distill and bce loss: 1.0817183830119945\n",
            " distill and bce loss: 1.055027030132435\n",
            " distill and bce loss: 1.050350922125357\n",
            " distill and bce loss: 1.0455943334985662\n",
            " distill and bce loss: 1.029808024565379\n",
            " distill and bce loss: 1.0270863031899486\n",
            " distill and bce loss: 1.011215195611671\n",
            " distill and bce loss: 1.0193768541018169\n",
            " distill and bce loss: 1.0066065556473203\n",
            " distill and bce loss: 1.0127932241669408\n",
            " distill and bce loss: 1.0095591092551197\n",
            " distill and bce loss: 1.008007111372771\n",
            " distill and bce loss: 0.978956816373048\n",
            " distill and bce loss: 1.0012238489256964\n",
            " distill and bce loss: 0.9966625814084653\n",
            " distill and bce loss: 0.9785780211289724\n",
            " distill and bce loss: 0.9863070812490251\n",
            " distill and bce loss: 0.9870799205921315\n",
            " distill and bce loss: 0.9758503955823404\n",
            " distill and bce loss: 0.9730467884628861\n",
            " distill and bce loss: 0.9825584844306663\n",
            " distill and bce loss: 0.9734855658478208\n",
            " distill and bce loss: 0.9784556251985056\n",
            " distill and bce loss: 0.9740223796279343\n",
            " distill and bce loss: 0.9744926194349924\n",
            " distill and bce loss: 0.9669685198201073\n",
            " distill and bce loss: 0.9677011679719996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.5205\n",
            "NME Accuracy: 0.624 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[203 198 305 185 263 174 216 220 191 243]\n",
            "Number support vectors:2198\n",
            "[6, 44, 45, 13, 15, 16, 81, 82, 83, 59]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  4\n",
            "5000 1000\n",
            "Seen cls number:  50\n",
            " distill and bce loss: 3.254097788422196\n",
            " distill and bce loss: 2.1311866133301347\n",
            " distill and bce loss: 1.9856960927998577\n",
            " distill and bce loss: 1.8862862123383417\n",
            " distill and bce loss: 1.82649235813706\n",
            " distill and bce loss: 1.7853614599616439\n",
            " distill and bce loss: 1.7269755910944056\n",
            " distill and bce loss: 1.743011596026244\n",
            " distill and bce loss: 1.68942497615461\n",
            " distill and bce loss: 1.6308040574744895\n",
            " distill and bce loss: 1.6504127206625763\n",
            " distill and bce loss: 1.6363374061054654\n",
            " distill and bce loss: 1.5936880751892373\n",
            " distill and bce loss: 1.571196229369552\n",
            " distill and bce loss: 1.578847465691743\n",
            " distill and bce loss: 1.5496725616631684\n",
            " distill and bce loss: 1.5795476524918168\n",
            " distill and bce loss: 1.5446735995787162\n",
            " distill and bce loss: 1.5381705628501043\n",
            " distill and bce loss: 1.5127239006536979\n",
            " distill and bce loss: 1.5085741612646315\n",
            " distill and bce loss: 1.4885338787679319\n",
            " distill and bce loss: 1.4915132610886186\n",
            " distill and bce loss: 1.4872755739423964\n",
            " distill and bce loss: 1.4136216088577553\n",
            " distill and bce loss: 1.4574138853285048\n",
            " distill and bce loss: 1.4513883038803383\n",
            " distill and bce loss: 1.453531649377611\n",
            " distill and bce loss: 1.4145017553258825\n",
            " distill and bce loss: 1.4083886345227559\n",
            " distill and bce loss: 1.4079977075258892\n",
            " distill and bce loss: 1.4422998030980427\n",
            " distill and bce loss: 1.4291040654535647\n",
            " distill and bce loss: 1.41138490041097\n",
            " distill and bce loss: 1.410020720075678\n",
            " distill and bce loss: 1.4099035572122645\n",
            " distill and bce loss: 1.3919256417839616\n",
            " distill and bce loss: 1.3722082045343187\n",
            " distill and bce loss: 1.3935704628626506\n",
            " distill and bce loss: 1.394448732888257\n",
            " distill and bce loss: 1.2665380261562489\n",
            " distill and bce loss: 1.1622527937094371\n",
            " distill and bce loss: 1.1286911103460524\n",
            " distill and bce loss: 1.1006053410194538\n",
            " distill and bce loss: 1.0951651566558414\n",
            " distill and bce loss: 1.0850940302566245\n",
            " distill and bce loss: 1.079973370940597\n",
            " distill and bce loss: 1.069363209936354\n",
            " distill and bce loss: 1.064970557336454\n",
            " distill and bce loss: 1.0601786529576336\n",
            " distill and bce loss: 1.0564950274096594\n",
            " distill and bce loss: 1.0547448926501803\n",
            " distill and bce loss: 1.0482706361346774\n",
            " distill and bce loss: 1.0437437075155753\n",
            " distill and bce loss: 1.044532769256168\n",
            " distill and bce loss: 1.0326056491445612\n",
            " distill and bce loss: 1.0294862853156195\n",
            " distill and bce loss: 1.0253083617598922\n",
            " distill and bce loss: 1.034217917256885\n",
            " distill and bce loss: 1.0239092740747664\n",
            " distill and bce loss: 1.0247252627655312\n",
            " distill and bce loss: 1.0300609347996887\n",
            " distill and bce loss: 1.0168405804369185\n",
            " distill and bce loss: 1.0155710887025904\n",
            " distill and bce loss: 1.0058861043718126\n",
            " distill and bce loss: 1.028809599302433\n",
            " distill and bce loss: 1.0211337970362768\n",
            " distill and bce loss: 1.0152263851077468\n",
            " distill and bce loss: 1.0070439422572102\n",
            " distill and bce loss: 1.0164659420649211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.4674\n",
            "NME Accuracy: 0.5952 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[191 185 257 105 216  97 160 221 174 191]\n",
            "Number support vectors:1797\n",
            "[72, 41, 10, 79, 52, 20, 54, 91, 60, 31]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  5\n",
            "5000 1000\n",
            "Seen cls number:  60\n",
            " distill and bce loss: 3.365884047967416\n",
            " distill and bce loss: 2.4716462117654308\n",
            " distill and bce loss: 2.3043333689371743\n",
            " distill and bce loss: 2.1519201221289457\n",
            " distill and bce loss: 2.0879638150886253\n",
            " distill and bce loss: 2.0438124780301696\n",
            " distill and bce loss: 1.9968000385496352\n",
            " distill and bce loss: 1.9434442475990013\n",
            " distill and bce loss: 1.8921162375697382\n",
            " distill and bce loss: 1.8891604653111211\n",
            " distill and bce loss: 1.8910046175674156\n",
            " distill and bce loss: 1.8914706684924938\n",
            " distill and bce loss: 1.8316051099035475\n",
            " distill and bce loss: 1.8036814199553595\n",
            " distill and bce loss: 1.7726452902511314\n",
            " distill and bce loss: 1.737315175709901\n",
            " distill and bce loss: 1.7417074618516144\n",
            " distill and bce loss: 1.7250508997175429\n",
            " distill and bce loss: 1.7153109930179737\n",
            " distill and bce loss: 1.7127232926863212\n",
            " distill and bce loss: 1.6636159309634455\n",
            " distill and bce loss: 1.6490589181582134\n",
            " distill and bce loss: 1.6424431293099016\n",
            " distill and bce loss: 1.6504778155574091\n",
            " distill and bce loss: 1.6216523117489285\n",
            " distill and bce loss: 1.5949356842924047\n",
            " distill and bce loss: 1.5976824716285423\n",
            " distill and bce loss: 1.5732515652974446\n",
            " distill and bce loss: 1.601203284881733\n",
            " distill and bce loss: 1.6211928283726726\n",
            " distill and bce loss: 1.581271807352702\n",
            " distill and bce loss: 1.5781360334820218\n",
            " distill and bce loss: 1.557541670622649\n",
            " distill and bce loss: 1.5822437405586243\n",
            " distill and bce loss: 1.5560342382501673\n",
            " distill and bce loss: 1.5908850762579176\n",
            " distill and bce loss: 1.5437450806299846\n",
            " distill and bce loss: 1.550983519465835\n",
            " distill and bce loss: 1.5005194412337408\n",
            " distill and bce loss: 1.5314959839538291\n",
            " distill and bce loss: 1.3731755857114438\n",
            " distill and bce loss: 1.2562848771059956\n",
            " distill and bce loss: 1.2239175747942042\n",
            " distill and bce loss: 1.2159202761120267\n",
            " distill and bce loss: 1.1913118362426758\n",
            " distill and bce loss: 1.201080439267335\n",
            " distill and bce loss: 1.1722772496717948\n",
            " distill and bce loss: 1.1735335345621463\n",
            " distill and bce loss: 1.1705576380093892\n",
            " distill and bce loss: 1.1528251789234303\n",
            " distill and bce loss: 1.155003489167602\n",
            " distill and bce loss: 1.1467017409978089\n",
            " distill and bce loss: 1.1497781055944938\n",
            " distill and bce loss: 1.1394205821885004\n",
            " distill and bce loss: 1.1367744130116921\n",
            " distill and bce loss: 1.1432379219267104\n",
            " distill and bce loss: 1.139203804510611\n",
            " distill and bce loss: 1.141713277057365\n",
            " distill and bce loss: 1.123915480242835\n",
            " distill and bce loss: 1.1294524228131329\n",
            " distill and bce loss: 1.1237053164729365\n",
            " distill and bce loss: 1.119143314935543\n",
            " distill and bce loss: 1.1161721028663494\n",
            " distill and bce loss: 1.1188267623936687\n",
            " distill and bce loss: 1.132296288454974\n",
            " distill and bce loss: 1.1193332749384421\n",
            " distill and bce loss: 1.1159669072539717\n",
            " distill and bce loss: 1.1097411469176963\n",
            " distill and bce loss: 1.1123324511227783\n",
            " distill and bce loss: 1.12406019921656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.3943333333333333\n",
            "NME Accuracy: 0.5506666666666666 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952, 0.5506666666666666]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[157 186 224 113 140 254 234 243 285 214]\n",
            "Number support vectors:2050\n",
            "[96, 97, 98, 2, 64, 37, 66, 71, 14, 95]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  6\n",
            "5000 1000\n",
            "Seen cls number:  70\n",
            " distill and bce loss: 3.505869419486434\n",
            " distill and bce loss: 2.437945246696472\n",
            " distill and bce loss: 2.2336152019324125\n",
            " distill and bce loss: 2.15484669914952\n",
            " distill and bce loss: 2.008711850201642\n",
            " distill and bce loss: 1.9820825413421348\n",
            " distill and bce loss: 1.9349105556805928\n",
            " distill and bce loss: 1.8726396958033245\n",
            " distill and bce loss: 1.9267179369926453\n",
            " distill and bce loss: 1.830578362500226\n",
            " distill and bce loss: 1.8036262812437835\n",
            " distill and bce loss: 1.8135176610063624\n",
            " distill and bce loss: 1.7622901576536674\n",
            " distill and bce loss: 1.735177300594471\n",
            " distill and bce loss: 1.7190342545509338\n",
            " distill and bce loss: 1.7193612941989191\n",
            " distill and bce loss: 1.7019285692109003\n",
            " distill and bce loss: 1.6338742397449635\n",
            " distill and bce loss: 1.6488683532785486\n",
            " distill and bce loss: 1.6874140258188601\n",
            " distill and bce loss: 1.7136289411120944\n",
            " distill and bce loss: 1.6606753843801993\n",
            " distill and bce loss: 1.6025575774687308\n",
            " distill and bce loss: 1.627048600603033\n",
            " distill and bce loss: 1.634480756741983\n",
            " distill and bce loss: 1.5933173210532576\n",
            " distill and bce loss: 1.618998905022939\n",
            " distill and bce loss: 1.6245229575369093\n",
            " distill and bce loss: 1.5958960056304932\n",
            " distill and bce loss: 1.6029094832914847\n",
            " distill and bce loss: 1.6180007656415303\n",
            " distill and bce loss: 1.5480684130280107\n",
            " distill and bce loss: 1.5858929068953902\n",
            " distill and bce loss: 1.5553883954330727\n",
            " distill and bce loss: 1.5448338543927227\n",
            " distill and bce loss: 1.5296879587350067\n",
            " distill and bce loss: 1.5506066061832287\n",
            " distill and bce loss: 1.5283843124354328\n",
            " distill and bce loss: 1.551624019940694\n",
            " distill and bce loss: 1.557158077204669\n",
            " distill and bce loss: 1.3967466199839558\n",
            " distill and bce loss: 1.2873712204120777\n",
            " distill and bce loss: 1.2648060034822535\n",
            " distill and bce loss: 1.2525080641110737\n",
            " distill and bce loss: 1.24052052365409\n",
            " distill and bce loss: 1.2189691927697923\n",
            " distill and bce loss: 1.2125827184429876\n",
            " distill and bce loss: 1.2045976740342599\n",
            " distill and bce loss: 1.2140101967034516\n",
            " distill and bce loss: 1.2099271924407393\n",
            " distill and bce loss: 1.2015116545889113\n",
            " distill and bce loss: 1.1922139282579776\n",
            " distill and bce loss: 1.1912435160742865\n",
            " distill and bce loss: 1.1837037702401478\n",
            " distill and bce loss: 1.1808655659357707\n",
            " distill and bce loss: 1.176622384124332\n",
            " distill and bce loss: 1.1801631329236206\n",
            " distill and bce loss: 1.182001989196848\n",
            " distill and bce loss: 1.1684520917910117\n",
            " distill and bce loss: 1.1759762874356023\n",
            " distill and bce loss: 1.1562796676600422\n",
            " distill and bce loss: 1.1735254029432933\n",
            " distill and bce loss: 1.1546726822853088\n",
            " distill and bce loss: 1.1740406420495775\n",
            " distill and bce loss: 1.1635213405997664\n",
            " distill and bce loss: 1.1604505115085177\n",
            " distill and bce loss: 1.1648728141078242\n",
            " distill and bce loss: 1.170618055043397\n",
            " distill and bce loss: 1.1667782664299011\n",
            " distill and bce loss: 1.162520580821567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.3525714285714286\n",
            "NME Accuracy: 0.5202857142857142 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952, 0.5506666666666666, 0.5202857142857142]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[194 257 230 239 184 218 183 208 235 131]\n",
            "Number support vectors:2079\n",
            "[0, 34, 35, 99, 42, 21, 86, 22, 24, 87]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  7\n",
            "5000 1000\n",
            "Seen cls number:  80\n",
            " distill and bce loss: 3.4736189753920943\n",
            " distill and bce loss: 2.3327583339479236\n",
            " distill and bce loss: 2.1648342278268604\n",
            " distill and bce loss: 2.101125591331058\n",
            " distill and bce loss: 1.9753517066990887\n",
            " distill and bce loss: 1.9200021514186152\n",
            " distill and bce loss: 1.884809390262321\n",
            " distill and bce loss: 1.8646879549379702\n",
            " distill and bce loss: 1.838862348485876\n",
            " distill and bce loss: 1.8124935538680464\n",
            " distill and bce loss: 1.8136457867092557\n",
            " distill and bce loss: 1.7785928028601188\n",
            " distill and bce loss: 1.7398519251081679\n",
            " distill and bce loss: 1.6986931142983612\n",
            " distill and bce loss: 1.7151075292516638\n",
            " distill and bce loss: 1.6995709052792303\n",
            " distill and bce loss: 1.6964598695437114\n",
            " distill and bce loss: 1.6947919505613822\n",
            " distill and bce loss: 1.6660118588694819\n",
            " distill and bce loss: 1.63772612368619\n",
            " distill and bce loss: 1.636206724025585\n",
            " distill and bce loss: 1.6401319194723059\n",
            " distill and bce loss: 1.6273539088390492\n",
            " distill and bce loss: 1.6304212587851066\n",
            " distill and bce loss: 1.6465208177213315\n",
            " distill and bce loss: 1.5906017378524497\n",
            " distill and bce loss: 1.58705249759886\n",
            " distill and bce loss: 1.5553817042598017\n",
            " distill and bce loss: 1.5278238477530304\n",
            " distill and bce loss: 1.522811472415924\n",
            " distill and bce loss: 1.5545578753506695\n",
            " distill and bce loss: 1.5515157447920904\n",
            " distill and bce loss: 1.585342162185245\n",
            " distill and bce loss: 1.5597161761036626\n",
            " distill and bce loss: 1.5373549373061568\n",
            " distill and bce loss: 1.549902198491273\n",
            " distill and bce loss: 1.5394090480274625\n",
            " distill and bce loss: 1.532746601987768\n",
            " distill and bce loss: 1.4705281257629395\n",
            " distill and bce loss: 1.477439370420244\n",
            " distill and bce loss: 1.3017670136910897\n",
            " distill and bce loss: 1.2325291456999603\n",
            " distill and bce loss: 1.1941157711876764\n",
            " distill and bce loss: 1.1776863159956756\n",
            " distill and bce loss: 1.1636893031773743\n",
            " distill and bce loss: 1.1644998501848292\n",
            " distill and bce loss: 1.141049353060899\n",
            " distill and bce loss: 1.1316224745026342\n",
            " distill and bce loss: 1.1346251335408952\n",
            " distill and bce loss: 1.1325326727496252\n",
            " distill and bce loss: 1.1295835331634239\n",
            " distill and bce loss: 1.1154427197244432\n",
            " distill and bce loss: 1.127378589577145\n",
            " distill and bce loss: 1.1132263430842646\n",
            " distill and bce loss: 1.1102063832459625\n",
            " distill and bce loss: 1.1018568674723308\n",
            " distill and bce loss: 1.1039813028441534\n",
            " distill and bce loss: 1.0962691715470068\n",
            " distill and bce loss: 1.1028836965560913\n",
            " distill and bce loss: 1.0992887527854354\n",
            " distill and bce loss: 1.0909871801182076\n",
            " distill and bce loss: 1.1014517026918906\n",
            " distill and bce loss: 1.0913405848873987\n",
            " distill and bce loss: 1.0944183855145067\n",
            " distill and bce loss: 1.0863555747049827\n",
            " distill and bce loss: 1.0895350167044886\n",
            " distill and bce loss: 1.0922447619614777\n",
            " distill and bce loss: 1.092830393049452\n",
            " distill and bce loss: 1.0910230775674183\n",
            " distill and bce loss: 1.0830261243714228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.28275\n",
            "NME Accuracy: 0.473125 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952, 0.5506666666666666, 0.5202857142857142, 0.473125]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[182 239 266 137 256 217 122 265 186 266]\n",
            "Number support vectors:2136\n",
            "[11, 12, 46, 47, 18, 30, 88, 25, 27, 94]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  8\n",
            "5000 1000\n",
            "Seen cls number:  90\n",
            " distill and bce loss: 3.530776346171344\n",
            " distill and bce loss: 2.4625484855086714\n",
            " distill and bce loss: 2.281572527355618\n",
            " distill and bce loss: 2.177029110767223\n",
            " distill and bce loss: 2.124586820602417\n",
            " distill and bce loss: 2.0704288019074335\n",
            " distill and bce loss: 2.0296633927910417\n",
            " distill and bce loss: 1.9845779207017686\n",
            " distill and bce loss: 1.901221114176291\n",
            " distill and bce loss: 1.9163219663831923\n",
            " distill and bce loss: 1.8898346181269046\n",
            " distill and bce loss: 1.8404405183262296\n",
            " distill and bce loss: 1.848975830607944\n",
            " distill and bce loss: 1.8360572655995686\n",
            " distill and bce loss: 1.806904011302524\n",
            " distill and bce loss: 1.8142953272219058\n",
            " distill and bce loss: 1.7379318475723267\n",
            " distill and bce loss: 1.7255941386576052\n",
            " distill and bce loss: 1.7474263001371313\n",
            " distill and bce loss: 1.7486890421973333\n",
            " distill and bce loss: 1.7699848590073761\n",
            " distill and bce loss: 1.7618860337469313\n",
            " distill and bce loss: 1.7256843050320942\n",
            " distill and bce loss: 1.7044292357232835\n",
            " distill and bce loss: 1.7104242267432037\n",
            " distill and bce loss: 1.6641120579507616\n",
            " distill and bce loss: 1.6726070797001873\n",
            " distill and bce loss: 1.6681249362451058\n",
            " distill and bce loss: 1.6517354404484783\n",
            " distill and bce loss: 1.6322970412395619\n",
            " distill and bce loss: 1.6327696862044159\n",
            " distill and bce loss: 1.6338009944668523\n",
            " distill and bce loss: 1.6692081027560763\n",
            " distill and bce loss: 1.642346481482188\n",
            " distill and bce loss: 1.6786292394002278\n",
            " distill and bce loss: 1.6311733303246674\n",
            " distill and bce loss: 1.6299420528941684\n",
            " distill and bce loss: 1.6220843129687839\n",
            " distill and bce loss: 1.6167116783283375\n",
            " distill and bce loss: 1.611789983731729\n",
            " distill and bce loss: 1.4529756638738844\n",
            " distill and bce loss: 1.3373260542198464\n",
            " distill and bce loss: 1.3066411857251767\n",
            " distill and bce loss: 1.2992828775335241\n",
            " distill and bce loss: 1.2787201801935832\n",
            " distill and bce loss: 1.2836030412603308\n",
            " distill and bce loss: 1.2746467192967732\n",
            " distill and bce loss: 1.2630873410790056\n",
            " distill and bce loss: 1.2529990628913596\n",
            " distill and bce loss: 1.251729636280625\n",
            " distill and bce loss: 1.2514731773623713\n",
            " distill and bce loss: 1.2319424593890156\n",
            " distill and bce loss: 1.2403841923784327\n",
            " distill and bce loss: 1.230934104433766\n",
            " distill and bce loss: 1.234493613243103\n",
            " distill and bce loss: 1.2193501459227667\n",
            " distill and bce loss: 1.2244570983780756\n",
            " distill and bce loss: 1.2299469554865803\n",
            " distill and bce loss: 1.2320887623009857\n",
            " distill and bce loss: 1.2209614647759333\n",
            " distill and bce loss: 1.2203517180901986\n",
            " distill and bce loss: 1.2117159454910844\n",
            " distill and bce loss: 1.2115010575011924\n",
            " distill and bce loss: 1.222030672762129\n",
            " distill and bce loss: 1.2232844432195027\n",
            " distill and bce loss: 1.2209049971015364\n",
            " distill and bce loss: 1.210037527260957\n",
            " distill and bce loss: 1.2199505192262154\n",
            " distill and bce loss: 1.2042386575981423\n",
            " distill and bce loss: 1.2150265508227878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.26122222222222224\n",
            "NME Accuracy: 0.4474444444444444 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952, 0.5506666666666666, 0.5202857142857142, 0.473125, 0.4474444444444444]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[166 167 236 194 247 235 197 201 301 273]\n",
            "Number support vectors:2217\n",
            "[32, 36, 69, 5, 7, 4, 75, 61, 62, 63]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  9\n",
            "5000 1000\n",
            "Seen cls number:  100\n",
            " distill and bce loss: 3.8898439230742277\n",
            " distill and bce loss: 2.6954109845338046\n",
            " distill and bce loss: 2.5324892821135343\n",
            " distill and bce loss: 2.3861313351878413\n",
            " distill and bce loss: 2.3243567016389637\n",
            " distill and bce loss: 2.304361895278648\n",
            " distill and bce loss: 2.2199422937852367\n",
            " distill and bce loss: 2.176952044169108\n",
            " distill and bce loss: 2.1705227251406067\n",
            " distill and bce loss: 2.126951486976058\n",
            " distill and bce loss: 2.0837407377031116\n",
            " distill and bce loss: 2.0922430908238447\n",
            " distill and bce loss: 2.0903013481034174\n",
            " distill and bce loss: 2.0334396914199546\n",
            " distill and bce loss: 2.0156426672582275\n",
            " distill and bce loss: 1.9997164607048035\n",
            " distill and bce loss: 1.9594176786917228\n",
            " distill and bce loss: 1.9888048745967724\n",
            " distill and bce loss: 1.9649567008018494\n",
            " distill and bce loss: 1.9574463720674868\n",
            " distill and bce loss: 1.9462071855862935\n",
            " distill and bce loss: 1.9166586553608929\n",
            " distill and bce loss: 1.926491587250321\n",
            " distill and bce loss: 1.9033820717423051\n",
            " distill and bce loss: 1.8826952223424558\n",
            " distill and bce loss: 1.8637110259797838\n",
            " distill and bce loss: 1.874135313210664\n",
            " distill and bce loss: 1.837878202950513\n",
            " distill and bce loss: 1.832175404937179\n",
            " distill and bce loss: 1.847665426907716\n",
            " distill and bce loss: 1.8589069777064853\n",
            " distill and bce loss: 1.8469988858258282\n",
            " distill and bce loss: 1.8204266539326421\n",
            " distill and bce loss: 1.849047135423731\n",
            " distill and bce loss: 1.8261824625509757\n",
            " distill and bce loss: 1.8223590475541573\n",
            " distill and bce loss: 1.8318060481989826\n",
            " distill and bce loss: 1.784188930635099\n",
            " distill and bce loss: 1.8251038745597556\n",
            " distill and bce loss: 1.8126161716602467\n",
            " distill and bce loss: 1.6139394618846752\n",
            " distill and bce loss: 1.5132277431311432\n",
            " distill and bce loss: 1.458144066510377\n",
            " distill and bce loss: 1.4563901799696464\n",
            " distill and bce loss: 1.4424194516959015\n",
            " distill and bce loss: 1.434003507649457\n",
            " distill and bce loss: 1.4105882313516405\n",
            " distill and bce loss: 1.4039875710452046\n",
            " distill and bce loss: 1.4104823757100988\n",
            " distill and bce loss: 1.3950499296188354\n",
            " distill and bce loss: 1.3784590297275119\n",
            " distill and bce loss: 1.401475965976715\n",
            " distill and bce loss: 1.3786780503061082\n",
            " distill and bce loss: 1.3764498079264607\n",
            " distill and bce loss: 1.369889380755248\n",
            " distill and bce loss: 1.3750194443596735\n",
            " distill and bce loss: 1.3781763602186132\n",
            " distill and bce loss: 1.3637140371181347\n",
            " distill and bce loss: 1.3574437476970531\n",
            " distill and bce loss: 1.359837086112411\n",
            " distill and bce loss: 1.3464048482753612\n",
            " distill and bce loss: 1.3517122401131525\n",
            " distill and bce loss: 1.348969558874766\n",
            " distill and bce loss: 1.3508771061897278\n",
            " distill and bce loss: 1.345187449896777\n",
            " distill and bce loss: 1.356999240539692\n",
            " distill and bce loss: 1.3459793329238892\n",
            " distill and bce loss: 1.3525863863803722\n",
            " distill and bce loss: 1.3529153753209997\n",
            " distill and bce loss: 1.3505418212325484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy SVC:0.2156\n",
            "NME Accuracy: 0.4188 \n",
            "*****************\n",
            "\n",
            "[0.882, 0.7415, 0.6976666666666667, 0.624, 0.5952, 0.5506666666666666, 0.5202857142857142, 0.473125, 0.4474444444444444, 0.4188]\n",
            " Selcting exemplars... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number support vectors per class:[289 159 278 319 256 302 240 196 198 237]\n",
            "Number support vectors:2474\n",
            "[33, 67, 39, 73, 48, 49, 29, 51, 57, 93]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}