{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICARL+classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Fi_hc_LgCe",
        "colab_type": "text"
      },
      "source": [
        "#import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcEloGsFLji8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d7462f79-048a-4e5d-b314-0c5e886d4322"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "import sys\n",
        "from torch.utils import *\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from torchvision.datasets import VisionDataset, CIFAR100\n",
        "import random\n",
        "from collections import ChainMap\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import skimage.io as io\n",
        "import glob\n",
        "import random\n",
        "import os.path\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torch.nn import MSELoss\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from __future__ import print_function, absolute_import\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "from sklearn.utils.validation import check_X_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.metrics import pairwise_distances_argmin_min,confusion_matrix\n",
        "import torchvision\n",
        "import seaborn as sns\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Scale, Resize, ToTensor, ToPILImage\n",
        "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss    \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.validation import _num_samples, check_array\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "import PIL.Image as Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from copy import deepcopy\n",
        "from scipy.spatial.distance import cosine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iZQttFWX7wI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "042cbe66-956a-44cb-9ffc-a7b873421362"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 26 14:20:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8     9W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFZpwOJ2LqRQ",
        "colab_type": "text"
      },
      "source": [
        "#dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uJpnYzRK4Mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1d17a9e1-a8ee-4106-869f-494bd0e87b43"
      },
      "source": [
        "np.random.seed(1993) \n",
        "order = np.arange(100)\n",
        "np.random.shuffle(order)\n",
        "print(order)\n",
        "label_map = {k: v for v, k in enumerate(order)}\n",
        "print(label_map)\n",
        "\n",
        "train = CIFAR100('.data/', train=True, transform=None, download=True)\n",
        "test = CIFAR100('.data/', train=False, transform=None, download=True)\n",
        "\n",
        "class Cifar100:\n",
        "    def __init__(self, train, test):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.train_groups, self.test_groups = self.split()\n",
        "        self.batch_num = 10\n",
        "\n",
        "    def split(self):\n",
        "        train_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.train:\n",
        "            if target in order[:10]:\n",
        "                train_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                train_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                train_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                train_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                train_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                train_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                train_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                train_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                train_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                train_groups[9].append((data,target))\n",
        "\n",
        "        test_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.test:\n",
        "            if target in order[:10]:\n",
        "                test_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                test_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                test_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                test_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                test_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                test_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                test_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                test_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                test_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                test_groups[9].append((data,target))\n",
        "\n",
        "        return train_groups, test_groups\n",
        "\n",
        "    def next_classes_batch(self, i):\n",
        "        return self.train_groups[i], self.test_groups[i]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cifar = Cifar100(train, test)\n",
        "    print(len(cifar.train_groups[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[68 56 78  8 23 84 90 65 74 76 40 89  3 92 55  9 26 80 43 38 58 70 77  1\n",
            " 85 19 17 50 28 53 13 81 45 82  6 59 83 16 15 44 91 41 72 60 79 52 20 10\n",
            " 31 54 37 95 14 71 96 98 97  2 64 66 42 22 35 86 24 34 87 21 99  0 88 27\n",
            " 18 94 11 12 47 25 30 46 62 69 36 61  7 63 75  5 32  4 51 48 73 93 39 67\n",
            " 29 49 57 33]\n",
            "{68: 0, 56: 1, 78: 2, 8: 3, 23: 4, 84: 5, 90: 6, 65: 7, 74: 8, 76: 9, 40: 10, 89: 11, 3: 12, 92: 13, 55: 14, 9: 15, 26: 16, 80: 17, 43: 18, 38: 19, 58: 20, 70: 21, 77: 22, 1: 23, 85: 24, 19: 25, 17: 26, 50: 27, 28: 28, 53: 29, 13: 30, 81: 31, 45: 32, 82: 33, 6: 34, 59: 35, 83: 36, 16: 37, 15: 38, 44: 39, 91: 40, 41: 41, 72: 42, 60: 43, 79: 44, 52: 45, 20: 46, 10: 47, 31: 48, 54: 49, 37: 50, 95: 51, 14: 52, 71: 53, 96: 54, 98: 55, 97: 56, 2: 57, 64: 58, 66: 59, 42: 60, 22: 61, 35: 62, 86: 63, 24: 64, 34: 65, 87: 66, 21: 67, 99: 68, 0: 69, 88: 70, 27: 71, 18: 72, 94: 73, 11: 74, 12: 75, 47: 76, 25: 77, 30: 78, 46: 79, 62: 80, 69: 81, 36: 82, 61: 83, 7: 84, 63: 85, 75: 86, 5: 87, 32: 88, 4: 89, 51: 90, 48: 91, 73: 92, 93: 93, 39: 94, 67: 95, 29: 96, 49: 97, 57: 98, 33: 99}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXd8eOYzLCKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchData(Dataset):\n",
        "    def __init__(self, images, labels, input_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.input_transform = input_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        image = Image.fromarray(np.uint8(image))\n",
        "        label = self.labels[index]\n",
        "        label = label_map[label]\n",
        "        if self.input_transform is not None:\n",
        "          image = self.input_transform(image)\n",
        "        \n",
        "        label = torch.LongTensor([label])\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EmbghpoMCLr",
        "colab_type": "text"
      },
      "source": [
        "#exemplar\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXnA52XLCxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Exemplar:\n",
        "    def __init__(self, max_size, total_cls): \n",
        "        self.train = {}\n",
        "        self.cur_cls = 0\n",
        "        self.max_size = max_size\n",
        "        self.total_classes = total_cls\n",
        "        self.store_num = {}\n",
        "        self.count_train = {}\n",
        "\n",
        "    def update(self, cls_num, train, inc_i):\n",
        "        train_x, train_y = train\n",
        "        cur_keys = list(set(train_y))\n",
        "        print(cur_keys)\n",
        "        def countX(tup, x): \n",
        "            count = 0\n",
        "            for ele in tup: \n",
        "              if (ele == x): \n",
        "                count = count + 1\n",
        "            return count \n",
        "        \n",
        "        self.cur_cls = cls_num\n",
        "\n",
        "        for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
        "            self.count_train[i] = countX(train_y, i)     \n",
        "            self.store_num[i] = int(self.count_train[i] / ((self.cur_cls)*0.1))\n",
        "\n",
        "        total_store_num = self.max_size / self.cur_cls #if self.cur_cls != 0 else max_size\n",
        "        train_store_num = int(total_store_num)  # ha senso?\n",
        "\n",
        "        for x, y in zip(train_x, train_y):\n",
        "            if y not in self.train:\n",
        "                self.train[y] = [x]\n",
        "            else:\n",
        "                if len(self.train[y]) < train_store_num:\n",
        "                    self.train[y].append(x)\n",
        "        assert self.cur_cls == len(list(self.train.keys()))\n",
        "        \n",
        "        for key, value in self.train.items():\n",
        "            self.train[key] = value[:train_store_num]\n",
        "        for key, value in self.train.items():\n",
        "            assert len(self.train[key]) == train_store_num      # ==!\n",
        "\n",
        "\n",
        "    def get_exemplar_train(self):\n",
        "        exemplar_train_x = []\n",
        "        exemplar_train_y = []\n",
        "        for key, value in self.train.items():\n",
        "            for train_x in value:\n",
        "                exemplar_train_x.append(train_x)\n",
        "                exemplar_train_y.append(key)\n",
        "        return exemplar_train_x, exemplar_train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgh0EvzAMTwQ",
        "colab_type": "text"
      },
      "source": [
        "#resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekEL47QWLHUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        self.old_model = None\n",
        "        self.num_classes = num_classes\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc = nn.Linear(64 * block.expansion, self.num_classes)\n",
        "  \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, features=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        if features:\n",
        "            x = x / x.norm()\n",
        "        else:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0u8iO2aMmrU",
        "colab_type": "text"
      },
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g7bt6chLKyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(self, total_cls):\n",
        "        self.total_cls = total_cls\n",
        "        self.seen_cls = 0\n",
        "        self.dataset = Cifar100(train, test)\n",
        "        self.model = resnet32(num_classes = total_cls).cuda()\n",
        "        self.count_label = {}\n",
        "        self.means = {}\n",
        "        transform_train = [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "        ]\n",
        "        transform_train.extend([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "        self.input_transform = Compose(transform_train)\n",
        "        \n",
        "        self.input_transform_eval = Compose([\n",
        "                                ToTensor(),\n",
        "                                Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010))])\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"Solver total trainable parameters : \", total_params)\n",
        "\n",
        "\n",
        "    def test(self, testdata):\n",
        "        self.model.eval()\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "        with torch.no_grad():\n",
        "          for i, (image, label) in enumerate(testdata):\n",
        "              image = image.cuda()\n",
        "              label = label.view(-1).cuda()\n",
        "              p = self.model(image)\n",
        "              pred = p[:,:self.seen_cls].argmax(dim=-1)\n",
        "              correct += sum(pred == label).item()\n",
        "              wrong += sum(pred != label).item()\n",
        "        acc = correct / (wrong + correct)\n",
        "        print(f\"\\r Test Acc: {acc*100} \\n\")\n",
        "        return acc\n",
        "\n",
        "    def NMEClassifier(self, test_dataloader, train_loader):\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in range(0,self.seen_cls):\n",
        "          t=0\n",
        "          mean = torch.zeros((1,64),device='cuda')\n",
        "          for indices,(images,labels) in enumerate((train_loader)):\n",
        "            images = images.to('cuda')\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,key in zip(outputs,labels):\n",
        "              if i==key:\n",
        "                mean+=output\n",
        "                t+=1\n",
        "          mean = mean/t\n",
        "          self.means[i] = mean / mean.norm()\n",
        "        running_corrects = 0\n",
        "        y_for_mat,y2_for_mat=[],[]\n",
        "        for images,labels in test_dataloader:\n",
        "            images = images.to('cuda')\n",
        "            labels = labels.to('cuda')\n",
        "            features = self.model(images,features=True)\n",
        "            for i,sample in enumerate(features):\n",
        "                c=[]\n",
        "                for k,v in self.means.items():\n",
        "                  temp=torch.dot(torch.squeeze(v), sample).data\n",
        "                  c.append(temp)\n",
        "                matx=torch.tensor(c)\n",
        "                y_pred = torch.argmax(matx).item()\n",
        "                y_for_mat.append(y_pred)\n",
        "                y2_for_mat.append(labels[i].cpu().numpy())\n",
        "                if y_pred == labels[i] : \n",
        "                    running_corrects+=1\n",
        "        mat = confusion_matrix(y2_for_mat,y_for_mat)\n",
        "        accuracy_eval = running_corrects / (100*self.seen_cls)   \n",
        "        print(f'Accuracy on eval NME:{accuracy_eval*100}' )\n",
        "\n",
        "        return accuracy_eval,mat\n",
        "\n",
        "    def CSClassifier(self, test_dataloader, train_loader):\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in range(0,self.seen_cls):\n",
        "          t=0\n",
        "          mean = torch.zeros((1,64),device='cuda')\n",
        "          for indices,(images,labels) in enumerate((train_loader)):\n",
        "            images = images.to('cuda')\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,key in zip(outputs,labels):\n",
        "              if i==key:\n",
        "                mean+=output\n",
        "                t+=1\n",
        "          mean = mean/t\n",
        "          self.means[i] = mean / mean.norm()\n",
        "        correct = 0.0\n",
        "        mat_lab,mat_pred=[],[]\n",
        "        for images, labels in test_dataloader:\n",
        "          images = images.to('cuda')\n",
        "          outputs = self.model(images,features=True)\n",
        "          preds = []\n",
        "          for output in outputs:\n",
        "            min_d = 99999999999999\n",
        "            for k,v in self.means.items():\n",
        "              dist=cosine(v.cpu(),output.cpu())\n",
        "              if dist<min_d:\n",
        "                min_d=dist\n",
        "                pred=k\n",
        "            preds.append(pred)\n",
        "          mat_lab+=list(labels.cpu().numpy())\n",
        "          mat_pred+=preds\n",
        "          for label, pred in zip(labels,preds):\n",
        "            if label == pred:\n",
        "              correct += 1\n",
        "      \n",
        "      accuracy = correct/(self.seen_cls*100)\n",
        "      print(f'Cosine Similarity Accuracy: {accuracy*100}')\n",
        "      mat = confusion_matrix(mat_lab,mat_pred)\n",
        "      return accuracy,mat\n",
        "\n",
        "    def SVMClassifier(self,test_loader,train_loader):\n",
        "      self.model.eval()\n",
        "      x_train=[]\n",
        "      y_train=[]\n",
        "      x_test=[]\n",
        "      y_test=[]\n",
        "      with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate((train_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              output=output/output.norm()\n",
        "              x_train.append(np.array(output.to('cpu')))\n",
        "              y_train.append(np.array(label))\n",
        "        n_correct = 0.0\n",
        "        for i, (images, labels) in enumerate((test_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              output=output/output.norm()\n",
        "              x_test.append(np.array(output.to('cpu')))\n",
        "              y_test.append(np.array(label))\n",
        "      svc = SVC(random_state=42, C=1)\n",
        "      svc.fit(x_train, y_train)\n",
        "      y_pred=svc.predict(x_test)\n",
        "      acc= accuracy_score(y_test, y_pred)\n",
        "      mat = confusion_matrix(y_test,y_pred)\n",
        "      print(f'accuracy SVC:{acc*100}')\n",
        "      return acc,mat\n",
        "\n",
        "    def KNNClassifier(self,test_loader,train_loader):\n",
        "      self.model.eval()\n",
        "      x_train=[]\n",
        "      y_train=[]\n",
        "      x_test=[]\n",
        "      y_test=[]\n",
        "      with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate((train_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              output=output/output.norm()\n",
        "              x_train.append(np.array(output.to('cpu')))\n",
        "              y_train.append(np.array(label))\n",
        "        n_correct = 0.0\n",
        "        for i, (images, labels) in enumerate((test_loader)):\n",
        "          images = images.to('cuda')\n",
        "          with torch.no_grad():\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,label in zip(outputs,labels):\n",
        "              output=output/output.norm()\n",
        "              x_test.append(np.array(output.to('cpu')))\n",
        "              y_test.append(np.array(label))\n",
        "      clf = KNeighborsClassifier()\n",
        "      clf.fit(x_train, y_train)\n",
        "      y_pred=clf.predict(x_test)\n",
        "      acc= accuracy_score(y_test, y_pred)\n",
        "      mat = confusion_matrix(y_test,y_pred)\n",
        "      print(f'accuracy KNN:{acc*100}')\n",
        "      return acc,mat\n",
        "\n",
        "    def get_lr(self, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            return param_group['lr']\n",
        "\n",
        "    def train(self, batch_size, epoches, lr, max_size):\n",
        "        total_cls = self.total_cls\n",
        "        accs_nme=[]\n",
        "        accs_svm=[]\n",
        "        accs_KNN=[]\n",
        "        accs_cos=[]\n",
        "        previous_model = None\n",
        "        exemplar = Exemplar(max_size, total_cls)\n",
        "        dataset = self.dataset\n",
        "        test_xs = []\n",
        "        test_ys = []\n",
        "        train_xs = []\n",
        "        train_ys = []\n",
        "\n",
        "        for inc_i in range(dataset.batch_num):\n",
        "            print(50*'---')\n",
        "            print(\" Incremental batch num: \" , inc_i)\n",
        "            train, test = dataset.next_classes_batch(inc_i)\n",
        "            train_x, train_y = zip(*train)\n",
        "            test_x, test_y = zip(*test)\n",
        "            test_xs.extend(test_x)\n",
        "            test_ys.extend(test_y)\n",
        " \n",
        "\n",
        "            if inc_i > 0:\n",
        "               train_xs, train_ys = exemplar.get_exemplar_train()\n",
        "               ex_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform), batch_size=128, shuffle=False, drop_last=True)\n",
        "\n",
        "            train_xs.extend(train_x)\n",
        "            train_ys.extend(train_y)  \n",
        "\n",
        "            def countX(tup, x): \n",
        "              count = 0\n",
        "              for ele in tup: \n",
        "                if (ele == x): \n",
        "                  count = count + 1\n",
        "              return count \n",
        "        \n",
        "\n",
        "            for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
        "              self.count_label[i] = countX(train_ys, i)     \n",
        "\n",
        "            print(len(list(self.count_label.keys())))\n",
        "              \n",
        "            self.seen_cls = (total_cls//dataset.batch_num)*(inc_i + 1)\n",
        "            print(\"Seen cls number: \", self.seen_cls)                            \n",
        "\n",
        "            train_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "            test_data = DataLoader(BatchData(test_xs, test_ys, input_transform=self.input_transform_eval),\n",
        "                        batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9,  weight_decay=0.00001)\n",
        "            scheduler = MultiStepLR(optimizer, milestones=[49, 63], gamma=0.2)\n",
        "\n",
        "            for epoch in range(epoches):\n",
        "                cur_lr = self.get_lr(optimizer)\n",
        "                print(f\"\\r EPOCH:{epoch}, LR:{cur_lr}\", end='')\n",
        "                if inc_i > 0:\n",
        "                   self.distill(train_data, optimizer, inc_i)\n",
        "                   scheduler.step()\n",
        "                else:\n",
        "                   self.stage_1(train_data,optimizer)\n",
        "                   scheduler.step()\n",
        "\n",
        "            self.previous_model = deepcopy(self.model)\n",
        "            if inc_i > 0:\n",
        "              acc1,mat1= self.SVMClassifier(test_data,train_data)\n",
        "              acc2,mat2 = self.NMEClassifier(test_data,train_data)\n",
        "              acc3,mat3 = self.CSClassifier(test_data,train_data)\n",
        "              acc4,mat4 = self.KNNClassifier(test_data,train_data)\n",
        "              accs_svm.append(acc1)\n",
        "              accs_nme.append(acc2)\n",
        "              accs_cos.append(acc3)\n",
        "              accs_KNN.append(acc4)\n",
        "            else:\n",
        "              acc = self.test(test_data)\n",
        "              accs_svm.append(acc)\n",
        "              accs_nme.append(acc)\n",
        "              accs_cos.append(acc)\n",
        "              accs_KNN.append(acc)\n",
        "            if inc_i==9:\n",
        "              fig, ax = plt.subplots()\n",
        "              ax = sns.heatmap(mat1)\n",
        "              plt.show()\n",
        "              fig, ax = plt.subplots()\n",
        "              ax = sns.heatmap(mat2)\n",
        "              plt.show()\n",
        "              fig, ax = plt.subplots()\n",
        "              ax = sns.heatmap(mat3)\n",
        "              plt.show()\n",
        "              fig, ax = plt.subplots()\n",
        "              ax = sns.heatmap(mat4)\n",
        "              plt.show()\n",
        "\n",
        "            print(f'SVM:{accs_svm} and mean:{mean(accs_svm)}')\n",
        "            print(f'NME:{accs_nme}and mean:{mean(accs_nme)}')\n",
        "            print(f'COSINE SIMILARITY:{accs_cos}and mean:{mean(accs_cos)}')\n",
        "            print(f'KNN:{accs_KNN}and mean:{mean(accs_KNN)}')\n",
        "            \n",
        "            x_ex, y_ex = self.exemplar_selection(train_x, train_y)   \n",
        "            exemplar.update(self.seen_cls, (x_ex, y_ex), inc_i)\n",
        "\n",
        "\n",
        "         \n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(accs_svm)+1), accs_svm, label = 'Test accuracy SVM')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(accs_nme)+1), accs_nme, label = 'Test accuracy  NME')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(accs_cos)+1), accs_cos, label = 'Test accuracy cosine similarity')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(accs_KNN)+1), accs_KNN, label = 'Test accuracy  KNN')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def cod_one_hot(self, x):\n",
        "        batch_size = 128\n",
        "        y_one_hot = torch.zeros(len(x), self.seen_cls).cuda()\n",
        "        y_one_hot = y_one_hot.scatter(1,x.long().view(-1,1).cuda(),1).cuda()\n",
        "        return y_one_hot\n",
        "\n",
        "\n",
        "    def stage_1(self, train_data, optimizer):\n",
        "        self.model.train()\n",
        "        losses = []\n",
        "        for i, (image, label) in enumerate((train_data)):\n",
        "            image = image.cuda()\n",
        "            label = label.view(-1).cuda() \n",
        "            label = self.cod_one_hot(label)\n",
        "            p = self.model(image)\n",
        "            loss = nn.BCEWithLogitsLoss()(p[:,:self.seen_cls],label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def exemplar_selection(self,x,y):\n",
        "        set_class=list(set(y))\n",
        "        m=round(2000/self.seen_cls)\n",
        "        means = dict.fromkeys(set_class)\n",
        "        exemplars = dict.fromkeys(set_class)\n",
        "        map = dict.fromkeys(set_class)\n",
        "        for label in map:\n",
        "          map[label] =  []\n",
        "          exemplars[label] = []\n",
        "        for item,la in zip(x,y):\n",
        "          for label in map:\n",
        "            if la == label:\n",
        "              map[label].append(item)\n",
        "        self.model.eval()\n",
        "        X_exemplars=[]\n",
        "        y_exemplars=[]\n",
        "        for label in map:\n",
        "          mean = torch.zeros((1,64),device='cuda')\n",
        "          class_outputs = []\n",
        "          tr_y=[label]*len(map[label])\n",
        "          with torch.no_grad():\n",
        "            loader = DataLoader(BatchData(map[label],tr_y , input_transform=self.input_transform_eval),batch_size=128, shuffle=False,num_workers=4,drop_last=False)\n",
        "            for images, _ in loader:\n",
        "                images = images.to('cuda')\n",
        "                outputs = self.model(images,features=True)\n",
        "                for output in outputs:\n",
        "                    output = output.to('cuda')\n",
        "                    class_outputs.append(output.data.cpu().numpy())\n",
        "                    mean += output\n",
        "            mean = mean / len(map[label])\n",
        "            means[label] = mean / mean.norm()\n",
        "          feat_ex = []\n",
        "          features = np.array(class_outputs)\n",
        "          indexing= []\n",
        "          for i in range(m):\n",
        "            s=np.sum(feat_ex, axis=0)\n",
        "            exs_mean = (features + s)/(i+1) \n",
        "            exs_mean = F.normalize(torch.tensor(exs_mean).to('cuda'))\n",
        "            ind = np.argsort( np.sqrt( np.sum((means[label].cpu().numpy() - exs_mean.cpu().numpy())**2, axis=1) ) )\n",
        "            flag=True\n",
        "            for el in ind:\n",
        "              if (el not in indexing) and (flag==True):\n",
        "                indexing.append(el)\n",
        "                feat_ex.append(features[el])\n",
        "                X_exemplars.append(map[label][el])\n",
        "                y_exemplars.append(label)\n",
        "                flag=False\n",
        "            print(f'\\r{i}/{m} exemplars for class: {label} and {len(X_exemplars)} total exemplars ', end='')\n",
        "        return X_exemplars,y_exemplars\n",
        "\n",
        "    def distill(self, train_data,optimizer, inc_i):\n",
        "            self.model.train()\n",
        "            for i, (image, label) in enumerate((train_data)):\n",
        "                image = image.cuda()\n",
        "                label = label.view(-1).cuda()\n",
        "                label_o = self.cod_one_hot(label)\n",
        "                p = self.model(image)\n",
        "                with torch.no_grad():\n",
        "                    pre_p = self.previous_model(image)\n",
        "                label_o[:,:self.seen_cls-10] =torch.sigmoid(pre_p[:,:self.seen_cls-10])\n",
        "                loss = nn.BCEWithLogitsLoss()(p[:,:self.seen_cls],label_o)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80M5tmGHWQni",
        "colab_type": "text"
      },
      "source": [
        "#main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NqenMALTFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Incremental Learning BIC')\n",
        "parser.add_argument('--batch_size', default = 128, type = int)\n",
        "parser.add_argument('--epoch', default = 70, type = int)\n",
        "parser.add_argument('--lr', default = 2, type = int)\n",
        "parser.add_argument('--max_size', default = 2000, type = int)\n",
        "parser.add_argument('--total_cls', default = 100, type = int)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "trainer = Trainer(args.total_cls)\n",
        "trainer.train(args.batch_size, args.epoch, args.lr, args.max_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}