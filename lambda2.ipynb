{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ewc_kmeans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMFTx_iVpWrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "17a9bc42-31be-432e-d918-de74e9ccd3b4"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision.datasets import VisionDataset, CIFAR100\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import ChainMap\n",
        "\n",
        "\n",
        "np.random.seed(1993) \n",
        "order = np.arange(100)\n",
        "np.random.shuffle(order)\n",
        "print(order)\n",
        "label_map = {k: v for v, k in enumerate(order)}\n",
        "\n",
        "\n",
        "\n",
        "print(label_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train = CIFAR100('.data/', train=True, transform=None, download=True)\n",
        "test = CIFAR100('.data/', train=False, transform=None, download=True)\n",
        "\n",
        "class Cifar100:\n",
        "    def __init__(self, train, test):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.train_groups, self.test_groups = self.split()\n",
        "        self.batch_num = 10\n",
        "\n",
        "    def split(self):\n",
        "        train_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.train:\n",
        "            if target in order[:10]:\n",
        "                train_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                train_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                train_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                train_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                train_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                train_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                train_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                train_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                train_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                train_groups[9].append((data,target))\n",
        "        assert len(train_groups[0]) == 5000, len(train_groups[0])\n",
        "        assert len(train_groups[1]) == 5000, len(train_groups[1])\n",
        "        assert len(train_groups[2]) == 5000, len(train_groups[2])\n",
        "        assert len(train_groups[3]) == 5000, len(train_groups[3])\n",
        "        assert len(train_groups[4]) == 5000, len(train_groups[4])\n",
        "        assert len(train_groups[5]) == 5000, len(train_groups[5])\n",
        "        assert len(train_groups[6]) == 5000, len(train_groups[6])\n",
        "        assert len(train_groups[7]) == 5000, len(train_groups[7])\n",
        "        assert len(train_groups[8]) == 5000, len(train_groups[8])\n",
        "        assert len(train_groups[9]) == 5000, len(train_groups[9])\n",
        "\n",
        "\n",
        "\n",
        "        test_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
        "        for data, target in self.test:\n",
        "            if target in order[:10]:\n",
        "                test_groups[0].append((data,target))\n",
        "            elif target in order[10:20]:\n",
        "                test_groups[1].append((data,target))\n",
        "            elif target in order[20:30]:\n",
        "                test_groups[2].append((data,target))\n",
        "            elif target in order[30:40]:\n",
        "                test_groups[3].append((data,target))\n",
        "            elif target in order[40:50]:\n",
        "                test_groups[4].append((data,target))\n",
        "            elif target in order[50:60]:\n",
        "                test_groups[5].append((data,target))\n",
        "            elif target in order[60:70]:\n",
        "                test_groups[6].append((data,target))\n",
        "            elif target in order[70:80]:\n",
        "                test_groups[7].append((data,target))\n",
        "            elif target in order[80:90]:\n",
        "                test_groups[8].append((data,target))\n",
        "            elif target in order[90:100]:\n",
        "                test_groups[9].append((data,target))\n",
        "        assert len(test_groups[0]) == 1000\n",
        "        assert len(test_groups[1]) == 1000\n",
        "        assert len(test_groups[2]) == 1000\n",
        "        assert len(test_groups[3]) == 1000\n",
        "        assert len(test_groups[4]) == 1000\n",
        "        assert len(test_groups[5]) == 1000\n",
        "        assert len(test_groups[6]) == 1000\n",
        "        assert len(test_groups[7]) == 1000\n",
        "        assert len(test_groups[8]) == 1000\n",
        "        assert len(test_groups[9]) == 1000\n",
        "\n",
        "        return train_groups, test_groups\n",
        "\n",
        "    def next_classes_batch(self, i):\n",
        "        return self.train_groups[i], self.test_groups[i]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cifar = Cifar100(train, test)\n",
        "    print(len(cifar.train_groups[0]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[68 56 78  8 23 84 90 65 74 76 40 89  3 92 55  9 26 80 43 38 58 70 77  1\n",
            " 85 19 17 50 28 53 13 81 45 82  6 59 83 16 15 44 91 41 72 60 79 52 20 10\n",
            " 31 54 37 95 14 71 96 98 97  2 64 66 42 22 35 86 24 34 87 21 99  0 88 27\n",
            " 18 94 11 12 47 25 30 46 62 69 36 61  7 63 75  5 32  4 51 48 73 93 39 67\n",
            " 29 49 57 33]\n",
            "{68: 0, 56: 1, 78: 2, 8: 3, 23: 4, 84: 5, 90: 6, 65: 7, 74: 8, 76: 9, 40: 10, 89: 11, 3: 12, 92: 13, 55: 14, 9: 15, 26: 16, 80: 17, 43: 18, 38: 19, 58: 20, 70: 21, 77: 22, 1: 23, 85: 24, 19: 25, 17: 26, 50: 27, 28: 28, 53: 29, 13: 30, 81: 31, 45: 32, 82: 33, 6: 34, 59: 35, 83: 36, 16: 37, 15: 38, 44: 39, 91: 40, 41: 41, 72: 42, 60: 43, 79: 44, 52: 45, 20: 46, 10: 47, 31: 48, 54: 49, 37: 50, 95: 51, 14: 52, 71: 53, 96: 54, 98: 55, 97: 56, 2: 57, 64: 58, 66: 59, 42: 60, 22: 61, 35: 62, 86: 63, 24: 64, 34: 65, 87: 66, 21: 67, 99: 68, 0: 69, 88: 70, 27: 71, 18: 72, 94: 73, 11: 74, 12: 75, 47: 76, 25: 77, 30: 78, 46: 79, 62: 80, 69: 81, 36: 82, 61: 83, 7: 84, 63: 85, 75: 86, 5: 87, 32: 88, 4: 89, 51: 90, 48: 91, 73: 92, 93: 93, 39: 94, 67: 95, 29: 96, 49: 97, 57: 98, 33: 99}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwMuXfutph8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import skimage.io as io\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "class BatchData(Dataset):\n",
        "    def __init__(self, images, labels, input_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.input_transform = input_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        image = Image.fromarray(np.uint8(image))\n",
        "        label = self.labels[index]\n",
        "        label = label_map[label]\n",
        "        if self.input_transform is not None:\n",
        "          image = self.input_transform(image)\n",
        "        \n",
        "        label = torch.LongTensor([label])\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJV7GMX8pkRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Exemplar:\n",
        "    def __init__(self, max_size, total_cls):    # serve total_cls?\n",
        "        self.train = {}\n",
        "        self.cur_cls = 0\n",
        "        self.max_size = max_size\n",
        "        self.total_classes = total_cls\n",
        "        self.store_num = {}\n",
        "        self.count_train = {}\n",
        "\n",
        "    def update(self, cls_num, train, inc_i):\n",
        "        train_x, train_y = train\n",
        "        cur_keys = list(set(train_y))\n",
        "        print(cur_keys)\n",
        "        def countX(tup, x): \n",
        "            count = 0\n",
        "            for ele in tup: \n",
        "              if (ele == x): \n",
        "                count = count + 1\n",
        "            return count \n",
        "        \n",
        "        self.cur_cls = cls_num\n",
        "\n",
        "        for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
        "            self.count_train[i] = countX(train_y, i)     \n",
        "            self.store_num[i] = int(self.count_train[i] / ((self.cur_cls)*0.1))\n",
        "            # print(i, self.store_num[i])    # to balance val new and val old\n",
        "\n",
        "\n",
        "        #assert self.cur_cls == len(list(self.train.keys()))\n",
        "\n",
        "        \n",
        "        total_store_num = self.max_size / self.cur_cls #if self.cur_cls != 0 else max_size\n",
        "        train_store_num = int(total_store_num)  # ha senso?\n",
        "\n",
        "        for x, y in zip(train_x, train_y):\n",
        "            if y not in self.train:\n",
        "                self.train[y] = [x]\n",
        "            else:\n",
        "                #if len(self.train[y]) < self.store_num[y]:\n",
        "                if len(self.train[y]) < train_store_num:\n",
        "                    self.train[y].append(x)\n",
        "        assert self.cur_cls == len(list(self.train.keys()))\n",
        "        \n",
        "        for key, value in self.train.items():\n",
        "            #self.store_num[key] = int(self.count_train[key] / ((self.cur_cls)*0.1))\n",
        "            #self.train[key] = value[:self.store_num[key]]\n",
        "            self.train[key] = value[:train_store_num]\n",
        "        for key, value in self.train.items():\n",
        "            #assert len(self.train[key]) == self.store_num[key]\n",
        "            assert len(self.train[key]) == train_store_num      # ==!\n",
        "\n",
        "\n",
        "    def get_exemplar_train(self):\n",
        "        exemplar_train_x = []\n",
        "        exemplar_train_y = []\n",
        "        for key, value in self.train.items():\n",
        "            for train_x in value:\n",
        "                exemplar_train_x.append(train_x)\n",
        "                exemplar_train_y.append(key)\n",
        "        return exemplar_train_x, exemplar_train_y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWE63gUpoKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jASHQHW4pqCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        self.old_model = None\n",
        "        self.num_classes = num_classes\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc = nn.Linear(64 * block.expansion, self.num_classes)\n",
        "  \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, features=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        if features:\n",
        "            x = x / x.norm()\n",
        "        else:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMOW7LKTptRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, absolute_import\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "from sklearn.utils.validation import check_X_y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9hwhiHpBiuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def variable(t: torch.Tensor, use_cuda=True, **kwargs):\n",
        "    if torch.cuda.is_available() and use_cuda:\n",
        "        t = t.cuda()\n",
        "    return Variable(t, **kwargs)\n",
        "\n",
        "class EWC(object):\n",
        "    def __init__(self, model, dataset, inc_i):\n",
        "\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.inc_i = inc_i\n",
        "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self._means = {}\n",
        "        self._precision_matrices = self._diag_fisher()\n",
        "\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            self._means[n] = variable(p.data)\n",
        "\n",
        "    def _diag_fisher(self):\n",
        "        precision_matrices = {}\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            p.data.zero_()\n",
        "            precision_matrices[n] = variable(p.data)\n",
        "\n",
        "        self.model.eval()\n",
        "        for _, (image, _) in enumerate((self.dataset)):\n",
        "            self.model.zero_grad()\n",
        "            image = variable(image)\n",
        "            output = self.model(image).view(1, -1)\n",
        "            label = output.max(1)[1].view(-1)\n",
        "            loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in self.model.named_parameters():\n",
        "                precision_matrices[n].data += p.grad.data ** 2 / (5000*(self.inc_i))\n",
        "\n",
        "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
        "        return precision_matrices\n",
        "\n",
        "    def penalty(self, model):\n",
        "        loss = 0\n",
        "        for n, p in model.named_parameters():\n",
        "            _loss = self._precision_matrices[n] * (p - self._means[n]) ** 2\n",
        "            loss += _loss.sum()\n",
        "        return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3xT3ftpz4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Scale, Resize, ToTensor, ToPILImage\n",
        "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss    \n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.validation import _num_samples, check_array\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "import numpy as np\n",
        "import glob\n",
        "import PIL.Image as Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "# import dml\n",
        "# from dml.ncmc import NCMC_Classifier\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, total_cls):\n",
        "        self.total_cls = total_cls\n",
        "        self.seen_cls = 0\n",
        "        self.dataset = Cifar100(train, test)\n",
        "        self.model = resnet32(num_classes = total_cls).cuda()\n",
        "        self.count_label = {}\n",
        "        samples_per_cls = [[0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100]\n",
        "        for inc_i in range(10):\n",
        "            assert len(samples_per_cls[inc_i]) == 100\n",
        "            for j in range(10):\n",
        "                samples_per_cls[inc_i][10*inc_i+j] = 500\n",
        "            for i in range(inc_i*10):\n",
        "                samples_per_cls[inc_i][i]=int(200/(inc_i))\n",
        "        beta = 0.999\n",
        "        effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
        "        weights = (1.0 - beta) / np.array(effective_num)\n",
        "        # weights = weights / np.sum(weights) * 100\n",
        "        self.weights = torch.Tensor(weights).cuda()\n",
        "        self.means = {}\n",
        "        transform_train = [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "        ]\n",
        "        transform_train.extend([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "        self.input_transform = Compose(transform_train)\n",
        "        \n",
        "        self.input_transform_eval = Compose([\n",
        "                                ToTensor(),\n",
        "                                Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010))])\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(\"Solver total trainable parameters : \", total_params)\n",
        "\n",
        "\n",
        "    def test(self, testdata):\n",
        "        # print(\"test data number : \",len(testdata))\n",
        "        self.model.eval()\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "        with torch.no_grad():\n",
        "          for i, (image, label) in enumerate(testdata):\n",
        "              image = image.cuda()\n",
        "              label = label.view(-1).cuda()\n",
        "              p = self.model(image)\n",
        "              pred = p[:,:self.seen_cls].argmax(dim=-1)\n",
        "              correct += sum(pred == label).item()\n",
        "              wrong += sum(pred != label).item()\n",
        "        acc = correct / (wrong + correct)\n",
        "        print(f\"\\r Test Acc: {acc*100} \\n\")\n",
        "        return acc\n",
        "\n",
        "    def NMEClassifier(self, test_dataloader, train_loader, display=True, suffix=''):\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in range(0,self.seen_cls):\n",
        "          t=0\n",
        "          mean = torch.zeros((1,64),device='cuda')\n",
        "          for indices,(images,labels) in enumerate((train_loader)):\n",
        "            images = images.to('cuda')\n",
        "            outputs = self.model(images,features=True)\n",
        "            for output,key in zip(outputs,labels):\n",
        "              if i==key:\n",
        "                mean+=output\n",
        "                t+=1\n",
        "          mean = mean/t\n",
        "          self.means[i] = mean / mean.norm()\n",
        "        running_corrects = 0\n",
        "        for images,labels in test_dataloader:\n",
        "            images = images.to('cuda')\n",
        "            labels = labels.to('cuda')\n",
        "            features = self.model(images,features=True)\n",
        "\n",
        "            for i,sample in enumerate(features):\n",
        "                dots = torch.tensor([torch.dot(torch.squeeze(mean), sample).data for _,mean in self.means.items()])\n",
        "                y_pred = torch.argmax(dots).item()\n",
        "                if y_pred == labels[i] : \n",
        "                    running_corrects+=1\n",
        "\n",
        "        accuracy_eval = running_corrects / (100*self.seen_cls)\n",
        "\n",
        "        if display :    \n",
        "            print('Accuracy on eval NME'+str(suffix)+':', accuracy_eval)\n",
        "\n",
        "        return accuracy_eval\n",
        "\n",
        "\n",
        "    def get_lr(self, optimizer):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            return param_group['lr']\n",
        "\n",
        "    def train(self, batch_size, epoches, lr, max_size):\n",
        "        total_cls = self.total_cls\n",
        "        criterion = CrossEntropyLoss()\n",
        "        # criterion = BCEWithLogitsLoss()\n",
        "\n",
        "        previous_model = None\n",
        "        exemplar = Exemplar(max_size, total_cls)\n",
        "        dataset = self.dataset\n",
        "        test_xs = []\n",
        "        test_ys = []\n",
        "        train_xs = []\n",
        "        train_ys = []\n",
        "\n",
        "        test_accs = []\n",
        "        for inc_i in range(dataset.batch_num):\n",
        "            print(50*'---')\n",
        "            print(\" Incremental batch num: \" , inc_i)\n",
        "            train, test = dataset.next_classes_batch(inc_i)\n",
        "            print(len(train), len(test))\n",
        "            train_x, train_y = zip(*train)\n",
        "            test_x, test_y = zip(*test)\n",
        "            test_xs.extend(test_x)\n",
        "            test_ys.extend(test_y)\n",
        " \n",
        "\n",
        "            if inc_i > 0:\n",
        "               old_task = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform), batch_size=128, shuffle=True, drop_last=True)\n",
        "               train_xs, train_ys = exemplar.get_exemplar_train()\n",
        "\n",
        "            \n",
        "            train_xs.extend(train_x)\n",
        "            train_ys.extend(train_y)  \n",
        "\n",
        "            def countX(tup, x): \n",
        "              count = 0\n",
        "              for ele in tup: \n",
        "                if (ele == x): \n",
        "                  count = count + 1\n",
        "              return count \n",
        "        \n",
        "\n",
        "            for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
        "              self.count_label[i] = countX(train_ys, i)     \n",
        "\n",
        "            print(len(list(self.count_label.keys())))\n",
        "              \n",
        "            self.seen_cls = (total_cls//dataset.batch_num)*(inc_i + 1)\n",
        "            print(\"Seen cls number: \", self.seen_cls)                            \n",
        "\n",
        "            assert self.seen_cls == len(list(self.count_label.keys()))    \n",
        "\n",
        "            train_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "            test_data = DataLoader(BatchData(test_xs, test_ys, input_transform=self.input_transform_eval),\n",
        "                        batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9,  weight_decay=2e-4)\n",
        "            # scheduler = LambdaLR(optimizer, lr_lambda=adjust_cifar100)\n",
        "            # scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
        "            scheduler = MultiStepLR(optimizer, milestones=[40, 60], gamma=0.2)\n",
        "\n",
        "\n",
        "            test_acc = []\n",
        "            \n",
        "\n",
        "            for epoch in range(epoches):\n",
        "                cur_lr = self.get_lr(optimizer)\n",
        "                print(f\"\\r EPOCH:{epoch}, LR:{cur_lr}\", end='')\n",
        "                if inc_i > 0:\n",
        "                   self.stage_ewc(train_data, old_task, criterion, optimizer, inc_i)\n",
        "                   scheduler.step()\n",
        "                else:\n",
        "                   self.stage_1(train_data, criterion, optimizer)\n",
        "                   scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "            self.previous_model = deepcopy(self.model)\n",
        "            if inc_i > 0:\n",
        "              acc = self.NMEClassifier(test_data,train_data)\n",
        "            else:\n",
        "              acc = self.test(test_data)\n",
        "            test_acc.append(acc)\n",
        "            test_accs.append(max(test_acc))\n",
        "            print(test_accs)\n",
        "            \n",
        "            x_ex, y_ex = self.exemplar_selection(train_x, train_y)   \n",
        "            exemplar.update(self.seen_cls, (x_ex, y_ex), inc_i)\n",
        "\n",
        "\n",
        "         \n",
        "        fig, ax = plt.subplots()\n",
        "        line1 = ax.plot(range(1, len(test_accs)+1), test_accs, label = 'Test accuracy')\n",
        "        ax.set(xlabel='N. Classes')\n",
        "        ax.grid()\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def cod_one_hot(self, x):\n",
        "        batch_size = 128\n",
        "        y_one_hot = torch.zeros(len(x), self.seen_cls).cuda()\n",
        "        y_one_hot = y_one_hot.scatter(1,x.long().view(-1,1).cuda(),1).cuda()\n",
        "        return y_one_hot\n",
        "\n",
        "\n",
        "    def stage_1(self, train_data, criterion, optimizer):\n",
        "        self.model.train()\n",
        "        print(f\"\\r Training new classes... \", end='')\n",
        "        losses = []\n",
        "        for i, (image, label) in enumerate((train_data)):\n",
        "            image = image.cuda()\n",
        "            label = label.view(-1).cuda() \n",
        "            # label = self.cod_one_hot(label)\n",
        "            p = self.model(image)\n",
        "            loss = criterion(p[:,:self.seen_cls], label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        print(f\"\\r stage1 loss: {np.mean(losses)} \\n\", end='')\n",
        "\n",
        "\n",
        "    def exemplar_selection(self,x,y):\n",
        "        print(f\"\\r Selecting exemplars... \", end='')\n",
        "\n",
        "        set_class=list(set(y))\n",
        "        m=round(2000/self.seen_cls)\n",
        "\n",
        "        # Initialize list of means, images and exemplars for each class\n",
        "        means = dict.fromkeys(set_class)\n",
        "        class_map = dict.fromkeys(set_class)\n",
        "        exemplars = dict.fromkeys(set_class)\n",
        "\n",
        "        for label in class_map:\n",
        "          class_map[label] =  []\n",
        "          exemplars[label] = []\n",
        "        # Fill class_map\n",
        "        for item,la in zip(x,y):\n",
        "          for label in class_map:\n",
        "            if la == label:\n",
        "              class_map[label].append(item)\n",
        "        # Get and save net outputs for each class\n",
        "        self.model.eval()\n",
        "        X_exemplars=[]\n",
        "        y_exemplars=[]\n",
        "        for label in class_map:\n",
        "          class_outputs = []\n",
        "          tr_y=[label]*len(class_map[label])\n",
        "          # Compute class means\n",
        "          with torch.no_grad():\n",
        "            loader = DataLoader(BatchData(class_map[label],tr_y , input_transform=self.input_transform_eval),batch_size=128, shuffle=False,num_workers=4,drop_last=False)\n",
        "            for images, _ in loader:\n",
        "                images = images.to('cuda')\n",
        "                outputs = self.model(images,features=True)\n",
        "                for output in outputs:\n",
        "                    output = output.to('cuda')\n",
        "                    class_outputs.append(output.data.cpu().numpy())\n",
        "          # Construct exemplar list for current class\n",
        "          features_exemplars = []\n",
        "          features = np.array(class_outputs)\n",
        "          features = F.normalize(torch.tensor(features))\n",
        "          kmeans = KMeans(n_clusters = m)\n",
        "          kmeans.fit(features)\n",
        "          i_added = []\n",
        "          for i in range(m):\n",
        "            i_vector = np.argmin( np.sqrt( np.sum( (torch.tensor(kmeans.cluster_centers_[i]).cpu().numpy() - features.cpu().numpy())**2, axis=1 ) ) )\n",
        "            features_exemplars.append(features[i_vector])\n",
        "            X_exemplars.append(class_map[label][i_vector])\n",
        "            y_exemplars.append(label)\n",
        "\n",
        "            print(f'\\r{i}/{m} exemplars for class: {label} and {len(X_exemplars)} total exemplars ', end='')\n",
        "        return X_exemplars, y_exemplars\n",
        "\n",
        "    def stage_ewc(self, train_data, old_task, criterion, optimizer, inc_i):\n",
        "        # ewc = EWC(self.model, old_task, inc_i)\n",
        "        self.model.train()\n",
        "        print(f\"\\r Training ... \", end='')\n",
        "        distill_losses = []\n",
        "        lambd = (10 * (inc_i)) / 100\n",
        "        for i, (image, label) in enumerate((train_data)):\n",
        "            image = image.cuda()\n",
        "            label = label.view(-1).cuda()\n",
        "            # label = self.cod_one_hot(label)\n",
        "            p = self.model(image)\n",
        "            with torch.no_grad():\n",
        "                pre_p = self.previous_model(image)\n",
        "            pre_p = F.softmax(pre_p[:,:self.seen_cls-10], dim=1)\n",
        "            logp = F.log_softmax(p[:,:self.seen_cls-10], dim=1)\n",
        "            dist_loss = -torch.mean(torch.sum(pre_p * logp, dim=1))\n",
        "            # ewc_loss = 100 * ewc.penalty(self.model)\n",
        "            loss_class = CrossEntropyLoss()(p[:,:self.seen_cls], label)\n",
        "            loss = (1-lambd) * loss_class + lambd * dist_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            distill_losses.append(loss.item())\n",
        "        print(f\"\\r distill and bce loss: {np.mean(distill_losses)}\\n\",end='')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chjnwr8quBK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0e8bfb98-b595-4054-f2bc-bc3189c19571"
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getoutput('nvidia-smi'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 18 16:52:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjiEjOFp34f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0e0a8cc-a9c8-4442-b5cf-a44dab0ae3a9"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "from torch.utils import *\n",
        "import argparse\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Incremental Learning BIC')\n",
        "parser.add_argument('--batch_size', default = 128, type = int)\n",
        "parser.add_argument('--epoch', default = 70, type = int)\n",
        "parser.add_argument('--lr', default = 0.1, type = int)\n",
        "parser.add_argument('--max_size', default = 2000, type = int)\n",
        "parser.add_argument('--total_cls', default = 100, type = int)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "trainer = Trainer(args.total_cls)\n",
        "trainer.train(args.batch_size, args.epoch, args.lr, args.max_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solver total trainable parameters :  472756\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  0\n",
            "5000 1000\n",
            "10\n",
            "Seen cls number:  10\n",
            " stage1 loss: 2.191917306337601 \n",
            " stage1 loss: 1.8364219879492736 \n",
            " stage1 loss: 1.6597462403468597 \n",
            " stage1 loss: 1.4979601792800121 \n",
            " stage1 loss: 1.3780383452391014 \n",
            " stage1 loss: 1.3125301752334986 \n",
            " stage1 loss: 1.208490749200185 \n",
            " stage1 loss: 1.1365887965911474 \n",
            " stage1 loss: 1.0916620783316784 \n",
            " stage1 loss: 1.0329595299867482 \n",
            " stage1 loss: 0.9707659589938629 \n",
            " stage1 loss: 0.9022580782572428 \n",
            " stage1 loss: 0.866198793435708 \n",
            " stage1 loss: 0.8053813775380453 \n",
            " stage1 loss: 0.7395222997054075 \n",
            " stage1 loss: 0.704314499329298 \n",
            " stage1 loss: 0.6706113670116816 \n",
            " stage1 loss: 0.632847480284862 \n",
            " stage1 loss: 0.6025533874829611 \n",
            " stage1 loss: 0.5678981290413783 \n",
            " stage1 loss: 0.5395660958228967 \n",
            " stage1 loss: 0.5126848488281934 \n",
            " stage1 loss: 0.47114420930544537 \n",
            " stage1 loss: 0.47094432360086685 \n",
            " stage1 loss: 0.44463958342870075 \n",
            " stage1 loss: 0.40895221936397064 \n",
            " stage1 loss: 0.39115867821069866 \n",
            " stage1 loss: 0.39438723753660154 \n",
            " stage1 loss: 0.376408040141448 \n",
            " stage1 loss: 0.38511109887025297 \n",
            " stage1 loss: 0.3504976893846805 \n",
            " stage1 loss: 0.3281202400342012 \n",
            " stage1 loss: 0.2819449477470838 \n",
            " stage1 loss: 0.3264869646384166 \n",
            " stage1 loss: 0.31387474750861144 \n",
            " stage1 loss: 0.28568495427950835 \n",
            " stage1 loss: 0.26060688228179246 \n",
            " stage1 loss: 0.2560942604755744 \n",
            " stage1 loss: 0.25740235509016574 \n",
            " stage1 loss: 0.2363975070990049 \n",
            " stage1 loss: 0.1688095931059275 \n",
            " stage1 loss: 0.11589277325532375 \n",
            " stage1 loss: 0.10652655047866014 \n",
            " stage1 loss: 0.08739324802389511 \n",
            " stage1 loss: 0.08952986205426547 \n",
            " stage1 loss: 0.0839579834196812 \n",
            " stage1 loss: 0.07247841596985474 \n",
            " stage1 loss: 0.06763516280513543 \n",
            " stage1 loss: 0.06526227911504415 \n",
            " stage1 loss: 0.06990812422755437 \n",
            " stage1 loss: 0.05999777313226309 \n",
            " stage1 loss: 0.05071917204902722 \n",
            " stage1 loss: 0.05489921039686753 \n",
            " stage1 loss: 0.05470897730153341 \n",
            " stage1 loss: 0.04822311072777479 \n",
            " stage1 loss: 0.04086331540766435 \n",
            " stage1 loss: 0.03926748123306494 \n",
            " stage1 loss: 0.03960013881516762 \n",
            " stage1 loss: 0.03709284502726335 \n",
            " stage1 loss: 0.03731143503234936 \n",
            " stage1 loss: 0.031688530093584306 \n",
            " stage1 loss: 0.026441391079853743 \n",
            " stage1 loss: 0.0287766024374809 \n",
            " stage1 loss: 0.025115795624561798 \n",
            " stage1 loss: 0.029854421957563132 \n",
            " stage1 loss: 0.026862240085999172 \n",
            " stage1 loss: 0.02651015172402064 \n",
            " stage1 loss: 0.024377387924454152 \n",
            " stage1 loss: 0.027582288695833623 \n",
            " stage1 loss: 0.027287597314287454 \n",
            " Test Acc: 87.1 \n",
            "\n",
            "[0.871]\n",
            "199/200 exemplars for class: 90 and 2000 total exemplars [65, 68, 8, 74, 76, 78, 84, 23, 56, 90]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  1\n",
            "5000 1000\n",
            "20\n",
            "Seen cls number:  20\n",
            " distill and bce loss: 2.2840017963338783\n",
            " distill and bce loss: 1.5016836060418024\n",
            " distill and bce loss: 1.344449308183458\n",
            " distill and bce loss: 1.2594762157510828\n",
            " distill and bce loss: 1.1741968669273235\n",
            " distill and bce loss: 1.126228352387746\n",
            " distill and bce loss: 1.07790086556364\n",
            " distill and bce loss: 1.0468866449815255\n",
            " distill and bce loss: 0.9915139156359213\n",
            " distill and bce loss: 0.9728738731808133\n",
            " distill and bce loss: 0.9115981901133502\n",
            " distill and bce loss: 0.9041883382532332\n",
            " distill and bce loss: 0.8553381418740308\n",
            " distill and bce loss: 0.8291264496467732\n",
            " distill and bce loss: 0.8201980921957228\n",
            " distill and bce loss: 0.7907524340682559\n",
            " distill and bce loss: 0.760130955113305\n",
            " distill and bce loss: 0.7567485438452827\n",
            " distill and bce loss: 0.7206627207773703\n",
            " distill and bce loss: 0.7106044535283689\n",
            " distill and bce loss: 0.6847327346051181\n",
            " distill and bce loss: 0.683336443371243\n",
            " distill and bce loss: 0.6386681535729656\n",
            " distill and bce loss: 0.6237166976487195\n",
            " distill and bce loss: 0.6290956288576126\n",
            " distill and bce loss: 0.6244910916796437\n",
            " distill and bce loss: 0.5950272900086863\n",
            " distill and bce loss: 0.5814896799899913\n",
            " distill and bce loss: 0.5675314074313199\n",
            " distill and bce loss: 0.5790876271548094\n",
            " distill and bce loss: 0.513738806049029\n",
            " distill and bce loss: 0.5451482534408569\n",
            " distill and bce loss: 0.5371601084868113\n",
            " distill and bce loss: 0.5335559359303227\n",
            " distill and bce loss: 0.4891071142973723\n",
            " distill and bce loss: 0.5018104425183049\n",
            " distill and bce loss: 0.48711701068613267\n",
            " distill and bce loss: 0.43634427476812293\n",
            " distill and bce loss: 0.4392759049380267\n",
            " distill and bce loss: 0.43921272842972364\n",
            " distill and bce loss: 0.3222185629937384\n",
            " distill and bce loss: 0.24082140458954704\n",
            " distill and bce loss: 0.21764985113232224\n",
            " distill and bce loss: 0.19675521304210028\n",
            " distill and bce loss: 0.18496665330948653\n",
            " distill and bce loss: 0.18580500064072786\n",
            " distill and bce loss: 0.17456886724189477\n",
            " distill and bce loss: 0.1734636127948761\n",
            " distill and bce loss: 0.16382742452400703\n",
            " distill and bce loss: 0.1607832720986119\n",
            " distill and bce loss: 0.15577346859154878\n",
            " distill and bce loss: 0.14925153884622785\n",
            " distill and bce loss: 0.14817270498584817\n",
            " distill and bce loss: 0.15017338455827148\n",
            " distill and bce loss: 0.14727915506120082\n",
            " distill and bce loss: 0.14447857756857518\n",
            " distill and bce loss: 0.1412659037720274\n",
            " distill and bce loss: 0.13443610637828154\n",
            " distill and bce loss: 0.13997640847056\n",
            " distill and bce loss: 0.13349853980320472\n",
            " distill and bce loss: 0.1315667409863737\n",
            " distill and bce loss: 0.1303499563148728\n",
            " distill and bce loss: 0.12563058927103324\n",
            " distill and bce loss: 0.12168075171885667\n",
            " distill and bce loss: 0.12321851982010736\n",
            " distill and bce loss: 0.11969348001811239\n",
            " distill and bce loss: 0.12151211513965218\n",
            " distill and bce loss: 0.1207984338204066\n",
            " distill and bce loss: 0.12105685044769887\n",
            " distill and bce loss: 0.11819773291548093\n",
            "Accuracy on eval NME: 0.7705\n",
            "[0.871, 0.7705]\n",
            "99/100 exemplars for class: 92 and 1000 total exemplars [3, 38, 40, 9, 43, 80, 55, 89, 26, 92]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  2\n",
            "5000 1000\n",
            "30\n",
            "Seen cls number:  30\n",
            " distill and bce loss: 1.9215280303248652\n",
            " distill and bce loss: 1.0500171990306288\n",
            " distill and bce loss: 0.9219985019277643\n",
            " distill and bce loss: 0.8265708320670657\n",
            " distill and bce loss: 0.7723397502192745\n",
            " distill and bce loss: 0.7185494502385458\n",
            " distill and bce loss: 0.6964784772307785\n",
            " distill and bce loss: 0.667959459401943\n",
            " distill and bce loss: 0.6448958372628247\n",
            " distill and bce loss: 0.6134803184756527\n",
            " distill and bce loss: 0.6128215723567538\n",
            " distill and bce loss: 0.6035872609527023\n",
            " distill and bce loss: 0.5820372645501737\n",
            " distill and bce loss: 0.5792710168494118\n",
            " distill and bce loss: 0.5359790921211243\n",
            " distill and bce loss: 0.5344363842849378\n",
            " distill and bce loss: 0.5000630661293313\n",
            " distill and bce loss: 0.5022567218100583\n",
            " distill and bce loss: 0.4880547793927016\n",
            " distill and bce loss: 0.44405750930309296\n",
            " distill and bce loss: 0.4520489865982974\n",
            " distill and bce loss: 0.48152695375460164\n",
            " distill and bce loss: 0.4499845918681886\n",
            " distill and bce loss: 0.4547438538736767\n",
            " distill and bce loss: 0.4519111856266304\n",
            " distill and bce loss: 0.42380324061270114\n",
            " distill and bce loss: 0.44679835714675764\n",
            " distill and bce loss: 0.45218059089448714\n",
            " distill and bce loss: 0.40561210833213945\n",
            " distill and bce loss: 0.40586214926507735\n",
            " distill and bce loss: 0.376987150421849\n",
            " distill and bce loss: 0.3788391053676605\n",
            " distill and bce loss: 0.3913698411650128\n",
            " distill and bce loss: 0.397538721009537\n",
            " distill and bce loss: 0.3910276039882942\n",
            " distill and bce loss: 0.38767434380672594\n",
            " distill and bce loss: 0.3985335252903126\n",
            " distill and bce loss: 0.37399189670880634\n",
            " distill and bce loss: 0.3672009513333992\n",
            " distill and bce loss: 0.35580428496555044\n",
            " distill and bce loss: 0.2830933274494277\n",
            " distill and bce loss: 0.22767029178363304\n",
            " distill and bce loss: 0.22065184844864738\n",
            " distill and bce loss: 0.2104571599651266\n",
            " distill and bce loss: 0.20398580614063475\n",
            " distill and bce loss: 0.20523761130041546\n",
            " distill and bce loss: 0.1997582843458211\n",
            " distill and bce loss: 0.19518282347255284\n",
            " distill and bce loss: 0.19636401147754104\n",
            " distill and bce loss: 0.19435210277636847\n",
            " distill and bce loss: 0.19127570313436013\n",
            " distill and bce loss: 0.18924479103750652\n",
            " distill and bce loss: 0.18352558105080216\n",
            " distill and bce loss: 0.18864752738564103\n",
            " distill and bce loss: 0.18505593748004348\n",
            " distill and bce loss: 0.18226296840994446\n",
            " distill and bce loss: 0.1840137935898922\n",
            " distill and bce loss: 0.18136806151381246\n",
            " distill and bce loss: 0.17871418512529796\n",
            " distill and bce loss: 0.18159926682710648\n",
            " distill and bce loss: 0.17933686408731672\n",
            " distill and bce loss: 0.17969538933700985\n",
            " distill and bce loss: 0.176100537456848\n",
            " distill and bce loss: 0.17197341747857905\n",
            " distill and bce loss: 0.175428935499103\n",
            " distill and bce loss: 0.17577865774984713\n",
            " distill and bce loss: 0.17626008639732996\n",
            " distill and bce loss: 0.17149467959448142\n",
            " distill and bce loss: 0.17468179596794975\n",
            " distill and bce loss: 0.17714574767483604\n",
            "Accuracy on eval NME: 0.723\n",
            "[0.871, 0.7705, 0.723]\n",
            "66/67 exemplars for class: 28 and 670 total exemplars [1, 70, 77, 17, 50, 19, 53, 85, 58, 28]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  3\n",
            "5000 1000\n",
            "40\n",
            "Seen cls number:  40\n",
            " distill and bce loss: 1.8678769491336964\n",
            " distill and bce loss: 1.0378853678703308\n",
            " distill and bce loss: 0.9386707743008932\n",
            " distill and bce loss: 0.8752470866397575\n",
            " distill and bce loss: 0.8169774115085602\n",
            " distill and bce loss: 0.789653996626536\n",
            " distill and bce loss: 0.7632033670390094\n",
            " distill and bce loss: 0.7386140249393605\n",
            " distill and bce loss: 0.7027813869493978\n",
            " distill and bce loss: 0.7133609696670815\n",
            " distill and bce loss: 0.6804306043518914\n",
            " distill and bce loss: 0.6596793841432642\n",
            " distill and bce loss: 0.6401738027731577\n",
            " distill and bce loss: 0.6426552832126617\n",
            " distill and bce loss: 0.6222868241645672\n",
            " distill and bce loss: 0.5980972272378428\n",
            " distill and bce loss: 0.5963624318440756\n",
            " distill and bce loss: 0.5670323206318749\n",
            " distill and bce loss: 0.5497319902534838\n",
            " distill and bce loss: 0.5601316933278684\n",
            " distill and bce loss: 0.5509163075023227\n",
            " distill and bce loss: 0.5665101828398528\n",
            " distill and bce loss: 0.5462850216362212\n",
            " distill and bce loss: 0.5364478517461706\n",
            " distill and bce loss: 0.5272525206760124\n",
            " distill and bce loss: 0.5354707031338303\n",
            " distill and bce loss: 0.5201336283374716\n",
            " distill and bce loss: 0.49478164811929065\n",
            " distill and bce loss: 0.4953959357959253\n",
            " distill and bce loss: 0.4942706398389958\n",
            " distill and bce loss: 0.5058389002526248\n",
            " distill and bce loss: 0.5030252221557829\n",
            " distill and bce loss: 0.4845146067716457\n",
            " distill and bce loss: 0.45745859101966574\n",
            " distill and bce loss: 0.4721032712194655\n",
            " distill and bce loss: 0.4663635166706862\n",
            " distill and bce loss: 0.47336077966071943\n",
            " distill and bce loss: 0.45777102366641714\n",
            " distill and bce loss: 0.4433701656482838\n",
            " distill and bce loss: 0.4746642405236209\n",
            " distill and bce loss: 0.36629596959661553\n",
            " distill and bce loss: 0.3220346597609697\n",
            " distill and bce loss: 0.30559904983750097\n",
            " distill and bce loss: 0.3008106229481874\n",
            " distill and bce loss: 0.29408652363000093\n",
            " distill and bce loss: 0.2920487569989981\n",
            " distill and bce loss: 0.2859704268199426\n",
            " distill and bce loss: 0.284074064206194\n",
            " distill and bce loss: 0.27492905463333484\n",
            " distill and bce loss: 0.2796005449361271\n",
            " distill and bce loss: 0.2716631555446872\n",
            " distill and bce loss: 0.2765782243675656\n",
            " distill and bce loss: 0.2698341790172789\n",
            " distill and bce loss: 0.27444617450237274\n",
            " distill and bce loss: 0.2706664952415007\n",
            " distill and bce loss: 0.26882318241728675\n",
            " distill and bce loss: 0.2670675207067419\n",
            " distill and bce loss: 0.26388594811713256\n",
            " distill and bce loss: 0.2647861311281169\n",
            " distill and bce loss: 0.26384666589675126\n",
            " distill and bce loss: 0.2595387777244603\n",
            " distill and bce loss: 0.2614832950962914\n",
            " distill and bce loss: 0.25773141339973166\n",
            " distill and bce loss: 0.2574958875775337\n",
            " distill and bce loss: 0.2636576165203695\n",
            " distill and bce loss: 0.2538298898273044\n",
            " distill and bce loss: 0.2572552153357753\n",
            " distill and bce loss: 0.25892184895497783\n",
            " distill and bce loss: 0.2574237060767633\n",
            " distill and bce loss: 0.2548854224107884\n",
            "Accuracy on eval NME: 0.65925\n",
            "[0.871, 0.7705, 0.723, 0.65925]\n",
            "49/50 exemplars for class: 59 and 500 total exemplars [6, 44, 13, 45, 15, 16, 81, 82, 83, 59]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  4\n",
            "5000 1000\n",
            "50\n",
            "Seen cls number:  50\n",
            " distill and bce loss: 1.6607344150543213\n",
            " distill and bce loss: 0.880429043813988\n",
            " distill and bce loss: 0.7916417993881084\n",
            " distill and bce loss: 0.7455787735956686\n",
            " distill and bce loss: 0.7238865009060612\n",
            " distill and bce loss: 0.6981115539868673\n",
            " distill and bce loss: 0.6793300950968707\n",
            " distill and bce loss: 0.6553620585688839\n",
            " distill and bce loss: 0.6586549591135096\n",
            " distill and bce loss: 0.6338311511057394\n",
            " distill and bce loss: 0.6307724637013895\n",
            " distill and bce loss: 0.6124565524083597\n",
            " distill and bce loss: 0.6066489567359289\n",
            " distill and bce loss: 0.5931180389942946\n",
            " distill and bce loss: 0.5708962276026055\n",
            " distill and bce loss: 0.5821034444702996\n",
            " distill and bce loss: 0.5773872120512856\n",
            " distill and bce loss: 0.5583088867090367\n",
            " distill and bce loss: 0.5661552173120005\n",
            " distill and bce loss: 0.5604120603314152\n",
            " distill and bce loss: 0.5636748312800018\n",
            " distill and bce loss: 0.5486498579934791\n",
            " distill and bce loss: 0.5474032588579036\n",
            " distill and bce loss: 0.535981414494691\n",
            " distill and bce loss: 0.5235970638416432\n",
            " distill and bce loss: 0.5169856642131452\n",
            " distill and bce loss: 0.5241619282298617\n",
            " distill and bce loss: 0.5366944792094054\n",
            " distill and bce loss: 0.5231097737948099\n",
            " distill and bce loss: 0.5086401071813371\n",
            " distill and bce loss: 0.5209279667448115\n",
            " distill and bce loss: 0.4999124385692455\n",
            " distill and bce loss: 0.510437101677612\n",
            " distill and bce loss: 0.49142446562095926\n",
            " distill and bce loss: 0.5144677537458914\n",
            " distill and bce loss: 0.500680383156847\n",
            " distill and bce loss: 0.5091990519452978\n",
            " distill and bce loss: 0.521793731936702\n",
            " distill and bce loss: 0.5058780936179338\n",
            " distill and bce loss: 0.5094519252026523\n",
            " distill and bce loss: 0.43530049202618776\n",
            " distill and bce loss: 0.3909896910190582\n",
            " distill and bce loss: 0.3778270234664281\n",
            " distill and bce loss: 0.37224948185461537\n",
            " distill and bce loss: 0.3675919390387005\n",
            " distill and bce loss: 0.36274603892255713\n",
            " distill and bce loss: 0.35974303274242964\n",
            " distill and bce loss: 0.3600302007463243\n",
            " distill and bce loss: 0.3569397567598908\n",
            " distill and bce loss: 0.35842395546259703\n",
            " distill and bce loss: 0.34944888121551937\n",
            " distill and bce loss: 0.35086797453739027\n",
            " distill and bce loss: 0.35175982393600325\n",
            " distill and bce loss: 0.3452144668058113\n",
            " distill and bce loss: 0.34618899170999173\n",
            " distill and bce loss: 0.349296521809366\n",
            " distill and bce loss: 0.34734170028456934\n",
            " distill and bce loss: 0.34186090860101914\n",
            " distill and bce loss: 0.3400852762990528\n",
            " distill and bce loss: 0.3377407016577544\n",
            " distill and bce loss: 0.33419147740911553\n",
            " distill and bce loss: 0.3372952590386073\n",
            " distill and bce loss: 0.3357533012275343\n",
            " distill and bce loss: 0.3355346338616477\n",
            " distill and bce loss: 0.3352926053382732\n",
            " distill and bce loss: 0.3345238660220747\n",
            " distill and bce loss: 0.3353037707231663\n",
            " distill and bce loss: 0.3360325260846703\n",
            " distill and bce loss: 0.33139499911555537\n",
            " distill and bce loss: 0.3308318869935142\n",
            "Accuracy on eval NME: 0.6256\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256]\n",
            "39/40 exemplars for class: 31 and 400 total exemplars [72, 41, 10, 79, 52, 20, 54, 91, 60, 31]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  5\n",
            "5000 1000\n",
            "60\n",
            "Seen cls number:  60\n",
            " distill and bce loss: 1.631682241404498\n",
            " distill and bce loss: 1.07137624974604\n",
            " distill and bce loss: 0.9826261964109209\n",
            " distill and bce loss: 0.9310669711342564\n",
            " distill and bce loss: 0.922714164963475\n",
            " distill and bce loss: 0.8827066178675051\n",
            " distill and bce loss: 0.855309267838796\n",
            " distill and bce loss: 0.8232212408825204\n",
            " distill and bce loss: 0.8299409393911008\n",
            " distill and bce loss: 0.8055162849249663\n",
            " distill and bce loss: 0.7985144423113929\n",
            " distill and bce loss: 0.7824744769820461\n",
            " distill and bce loss: 0.7565538596223902\n",
            " distill and bce loss: 0.7698575125800239\n",
            " distill and bce loss: 0.7658105360137092\n",
            " distill and bce loss: 0.7629564305146536\n",
            " distill and bce loss: 0.7268223022973096\n",
            " distill and bce loss: 0.7157709896564484\n",
            " distill and bce loss: 0.7178215130611703\n",
            " distill and bce loss: 0.7084988786114587\n",
            " distill and bce loss: 0.6946659330968503\n",
            " distill and bce loss: 0.6824626481091535\n",
            " distill and bce loss: 0.7029312330263632\n",
            " distill and bce loss: 0.6846299612963641\n",
            " distill and bce loss: 0.6943825715117984\n",
            " distill and bce loss: 0.6857320379327845\n",
            " distill and bce loss: 0.674220272788295\n",
            " distill and bce loss: 0.674478616979387\n",
            " distill and bce loss: 0.6738078748738324\n",
            " distill and bce loss: 0.6647199094295502\n",
            " distill and bce loss: 0.6737781796190474\n",
            " distill and bce loss: 0.6692105973208392\n",
            " distill and bce loss: 0.6803556515110863\n",
            " distill and bce loss: 0.6636131637626224\n",
            " distill and bce loss: 0.6481168601248\n",
            " distill and bce loss: 0.6575674017270406\n",
            " distill and bce loss: 0.650822839251271\n",
            " distill and bce loss: 0.6547230537290927\n",
            " distill and bce loss: 0.652063579471023\n",
            " distill and bce loss: 0.6386577972659359\n",
            " distill and bce loss: 0.5673595192255797\n",
            " distill and bce loss: 0.5227615176527588\n",
            " distill and bce loss: 0.5108854367777154\n",
            " distill and bce loss: 0.5039905806382498\n",
            " distill and bce loss: 0.49995651454837237\n",
            " distill and bce loss: 0.4965244917957871\n",
            " distill and bce loss: 0.49027980201774174\n",
            " distill and bce loss: 0.49158623649014366\n",
            " distill and bce loss: 0.48636328898094316\n",
            " distill and bce loss: 0.48805079857508343\n",
            " distill and bce loss: 0.48706975137745895\n",
            " distill and bce loss: 0.47890336204458167\n",
            " distill and bce loss: 0.47577718562550014\n",
            " distill and bce loss: 0.4765140138290547\n",
            " distill and bce loss: 0.4773017896546258\n",
            " distill and bce loss: 0.47435086248097597\n",
            " distill and bce loss: 0.4745815004463549\n",
            " distill and bce loss: 0.4716081955918559\n",
            " distill and bce loss: 0.46808988021479714\n",
            " distill and bce loss: 0.467144513019809\n",
            " distill and bce loss: 0.4711536743022777\n",
            " distill and bce loss: 0.46201588047875297\n",
            " distill and bce loss: 0.46376379055005534\n",
            " distill and bce loss: 0.45713326721279707\n",
            " distill and bce loss: 0.45774062253810743\n",
            " distill and bce loss: 0.4655726088417901\n",
            " distill and bce loss: 0.4652110890105919\n",
            " distill and bce loss: 0.46725324182598676\n",
            " distill and bce loss: 0.463626358796049\n",
            " distill and bce loss: 0.46259531875451404\n",
            "Accuracy on eval NME: 0.585\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256, 0.585]\n",
            "32/33 exemplars for class: 95 and 330 total exemplars [96, 97, 98, 64, 66, 2, 37, 71, 14, 95]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  6\n",
            "5000 1000\n",
            "70\n",
            "Seen cls number:  70\n",
            " distill and bce loss: 1.5903361196871157\n",
            " distill and bce loss: 1.1558812967053167\n",
            " distill and bce loss: 1.0180977638121005\n",
            " distill and bce loss: 0.998977056256047\n",
            " distill and bce loss: 0.9781090628217768\n",
            " distill and bce loss: 0.9362226647359354\n",
            " distill and bce loss: 0.9236574095708353\n",
            " distill and bce loss: 0.9255219322663767\n",
            " distill and bce loss: 0.904547580966243\n",
            " distill and bce loss: 0.8922587048124384\n",
            " distill and bce loss: 0.8689571232707413\n",
            " distill and bce loss: 0.8553049211148862\n",
            " distill and bce loss: 0.8479133281442854\n",
            " distill and bce loss: 0.8507928771001322\n",
            " distill and bce loss: 0.8311003424503185\n",
            " distill and bce loss: 0.8341808959289834\n",
            " distill and bce loss: 0.8311441088164294\n",
            " distill and bce loss: 0.8375092226046102\n",
            " distill and bce loss: 0.8182798326015472\n",
            " distill and bce loss: 0.8244224952326881\n",
            " distill and bce loss: 0.8234316110610962\n",
            " distill and bce loss: 0.8048752071680846\n",
            " distill and bce loss: 0.7941057902795298\n",
            " distill and bce loss: 0.7987310069578665\n",
            " distill and bce loss: 0.7903564428841626\n",
            " distill and bce loss: 0.7878678628691921\n",
            " distill and bce loss: 0.7802390334782777\n",
            " distill and bce loss: 0.7739973333146837\n",
            " distill and bce loss: 0.7713300707163634\n",
            " distill and bce loss: 0.7791276861120153\n",
            " distill and bce loss: 0.7832140337537836\n",
            " distill and bce loss: 0.7782968393078556\n",
            " distill and bce loss: 0.7883071590352941\n",
            " distill and bce loss: 0.7828882171048058\n",
            " distill and bce loss: 0.7754346562756432\n",
            " distill and bce loss: 0.7766241353970987\n",
            " distill and bce loss: 0.7611226351172836\n",
            " distill and bce loss: 0.7679487345395265\n",
            " distill and bce loss: 0.762787065020314\n",
            " distill and bce loss: 0.7602267618532534\n",
            " distill and bce loss: 0.6804179836202551\n",
            " distill and bce loss: 0.6455754891589836\n",
            " distill and bce loss: 0.6369200834521541\n",
            " distill and bce loss: 0.630550718969769\n",
            " distill and bce loss: 0.6299283957039868\n",
            " distill and bce loss: 0.6195370720492469\n",
            " distill and bce loss: 0.6131638873506475\n",
            " distill and bce loss: 0.6160300208462609\n",
            " distill and bce loss: 0.6104529875296133\n",
            " distill and bce loss: 0.6049851289501896\n",
            " distill and bce loss: 0.6070541474554274\n",
            " distill and bce loss: 0.611033100772787\n",
            " distill and bce loss: 0.6057725063076725\n",
            " distill and bce loss: 0.6000384743566867\n",
            " distill and bce loss: 0.5982167400695659\n",
            " distill and bce loss: 0.5963995727124037\n",
            " distill and bce loss: 0.6024250343993858\n",
            " distill and bce loss: 0.5967707148304692\n",
            " distill and bce loss: 0.5938340955310397\n",
            " distill and bce loss: 0.5965542478693856\n",
            " distill and bce loss: 0.5896107157071432\n",
            " distill and bce loss: 0.5893235543259868\n",
            " distill and bce loss: 0.5944328710988716\n",
            " distill and bce loss: 0.5830554658616031\n",
            " distill and bce loss: 0.5858497614109958\n",
            " distill and bce loss: 0.5860764593989761\n",
            " distill and bce loss: 0.5870862415543309\n",
            " distill and bce loss: 0.5888347300114455\n",
            " distill and bce loss: 0.5885060092917195\n",
            " distill and bce loss: 0.5867573530585678\n",
            "Accuracy on eval NME: 0.5562857142857143\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256, 0.585, 0.5562857142857143]\n",
            "28/29 exemplars for class: 24 and 290 total exemplars [0, 34, 99, 35, 42, 21, 86, 87, 22, 24]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  7\n",
            "5000 1000\n",
            "80\n",
            "Seen cls number:  80\n",
            " distill and bce loss: 1.5305744100500036\n",
            " distill and bce loss: 1.1785097298798737\n",
            " distill and bce loss: 1.098360879553689\n",
            " distill and bce loss: 1.0657392486378\n",
            " distill and bce loss: 1.0089325794467219\n",
            " distill and bce loss: 0.9802411441449765\n",
            " distill and bce loss: 0.9771369386602331\n",
            " distill and bce loss: 0.9512417945596907\n",
            " distill and bce loss: 0.9488756601457242\n",
            " distill and bce loss: 0.9396070782785062\n",
            " distill and bce loss: 0.9215040836069319\n",
            " distill and bce loss: 0.9202758570512136\n",
            " distill and bce loss: 0.9326804556228496\n",
            " distill and bce loss: 0.8946028528390108\n",
            " distill and bce loss: 0.8993540936046176\n",
            " distill and bce loss: 0.8828664940816385\n",
            " distill and bce loss: 0.8932306126311973\n",
            " distill and bce loss: 0.8868898164342951\n",
            " distill and bce loss: 0.871056295103497\n",
            " distill and bce loss: 0.8853386221108613\n",
            " distill and bce loss: 0.8743998595961818\n",
            " distill and bce loss: 0.8719826848418625\n",
            " distill and bce loss: 0.8717823459042443\n",
            " distill and bce loss: 0.8692921389032293\n",
            " distill and bce loss: 0.8758145846702434\n",
            " distill and bce loss: 0.8723787687442921\n",
            " distill and bce loss: 0.8584850794739194\n",
            " distill and bce loss: 0.8549983976063905\n",
            " distill and bce loss: 0.8709905048211416\n",
            " distill and bce loss: 0.8718685000031082\n",
            " distill and bce loss: 0.860939821711293\n",
            " distill and bce loss: 0.8545630905363295\n",
            " distill and bce loss: 0.8585799546153458\n",
            " distill and bce loss: 0.8444349191806935\n",
            " distill and bce loss: 0.8428899414009519\n",
            " distill and bce loss: 0.855608379399335\n",
            " distill and bce loss: 0.8536478866029669\n",
            " distill and bce loss: 0.8467514481809404\n",
            " distill and bce loss: 0.8537907655592318\n",
            " distill and bce loss: 0.8600258198049333\n",
            " distill and bce loss: 0.7687618357163889\n",
            " distill and bce loss: 0.717427874052966\n",
            " distill and bce loss: 0.7135343838621069\n",
            " distill and bce loss: 0.7047179330278326\n",
            " distill and bce loss: 0.6955078343550364\n",
            " distill and bce loss: 0.6908407961880719\n",
            " distill and bce loss: 0.6855675092449894\n",
            " distill and bce loss: 0.693901468206335\n",
            " distill and bce loss: 0.6799676749441359\n",
            " distill and bce loss: 0.6821319361527761\n",
            " distill and bce loss: 0.682832306181943\n",
            " distill and bce loss: 0.6733331823790515\n",
            " distill and bce loss: 0.6778290095152678\n",
            " distill and bce loss: 0.6723017703603815\n",
            " distill and bce loss: 0.6812005318977215\n",
            " distill and bce loss: 0.6717366841104295\n",
            " distill and bce loss: 0.6736776442439468\n",
            " distill and bce loss: 0.6711554372752154\n",
            " distill and bce loss: 0.6666619678338369\n",
            " distill and bce loss: 0.6677652641578957\n",
            " distill and bce loss: 0.6651813045695976\n",
            " distill and bce loss: 0.6638101913310863\n",
            " distill and bce loss: 0.6573461007188868\n",
            " distill and bce loss: 0.6553217481683802\n",
            " distill and bce loss: 0.6536849836508433\n",
            " distill and bce loss: 0.6496762379452035\n",
            " distill and bce loss: 0.6564788454108768\n",
            " distill and bce loss: 0.6502748231093088\n",
            " distill and bce loss: 0.6520701679918501\n",
            " distill and bce loss: 0.6562510519116013\n",
            "Accuracy on eval NME: 0.528875\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256, 0.585, 0.5562857142857143, 0.528875]\n",
            "24/25 exemplars for class: 94 and 250 total exemplars [11, 12, 46, 47, 18, 94, 88, 25, 27, 30]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  8\n",
            "5000 1000\n",
            "90\n",
            "Seen cls number:  90\n",
            " distill and bce loss: 1.5259665603990908\n",
            " distill and bce loss: 1.3823608733989574\n",
            " distill and bce loss: 1.291278854564384\n",
            " distill and bce loss: 1.2503100081726357\n",
            " distill and bce loss: 1.2236061957147386\n",
            " distill and bce loss: 1.194811604641102\n",
            " distill and bce loss: 1.1898213889863756\n",
            " distill and bce loss: 1.180212343180621\n",
            " distill and bce loss: 1.1680278579394023\n",
            " distill and bce loss: 1.1653678847683802\n",
            " distill and bce loss: 1.1536979300004464\n",
            " distill and bce loss: 1.1406740965666595\n",
            " distill and bce loss: 1.1443936383282696\n",
            " distill and bce loss: 1.122623794608646\n",
            " distill and bce loss: 1.1273297314290647\n",
            " distill and bce loss: 1.1346971370555736\n",
            " distill and bce loss: 1.1425458479810644\n",
            " distill and bce loss: 1.1191490402928106\n",
            " distill and bce loss: 1.113110132791378\n",
            " distill and bce loss: 1.1144518499021177\n",
            " distill and bce loss: 1.1173733163762976\n",
            " distill and bce loss: 1.1276948087745242\n",
            " distill and bce loss: 1.110668299374757\n",
            " distill and bce loss: 1.1107992938271276\n",
            " distill and bce loss: 1.1021158695220947\n",
            " distill and bce loss: 1.0958474267412115\n",
            " distill and bce loss: 1.1046377972320274\n",
            " distill and bce loss: 1.1019950409730275\n",
            " distill and bce loss: 1.109075571651812\n",
            " distill and bce loss: 1.1069104229962383\n",
            " distill and bce loss: 1.1096908979945712\n",
            " distill and bce loss: 1.0910225135308724\n",
            " distill and bce loss: 1.1012992991341486\n",
            " distill and bce loss: 1.1053075933897938\n",
            " distill and bce loss: 1.0993564360671573\n",
            " distill and bce loss: 1.0996331991972748\n",
            " distill and bce loss: 1.0945936055095107\n",
            " distill and bce loss: 1.094393323968958\n",
            " distill and bce loss: 1.0915604884977694\n",
            " distill and bce loss: 1.0888851594041895\n",
            " distill and bce loss: 1.0221882705335263\n",
            " distill and bce loss: 0.9787362703570613\n",
            " distill and bce loss: 0.9616938178186063\n",
            " distill and bce loss: 0.9726082042411521\n",
            " distill and bce loss: 0.956259141365687\n",
            " distill and bce loss: 0.9531743349852385\n",
            " distill and bce loss: 0.9405987075081578\n",
            " distill and bce loss: 0.9449339851185128\n",
            " distill and bce loss: 0.9365791281064352\n",
            " distill and bce loss: 0.9387627795890525\n",
            " distill and bce loss: 0.9357339872254266\n",
            " distill and bce loss: 0.9398873271765532\n",
            " distill and bce loss: 0.9365196459823184\n",
            " distill and bce loss: 0.9281956266473841\n",
            " distill and bce loss: 0.93542304303911\n",
            " distill and bce loss: 0.9275260070959727\n",
            " distill and bce loss: 0.9261302351951599\n",
            " distill and bce loss: 0.9238038140314596\n",
            " distill and bce loss: 0.9300560454527537\n",
            " distill and bce loss: 0.9276367514221756\n",
            " distill and bce loss: 0.9245086897302557\n",
            " distill and bce loss: 0.9127653649559727\n",
            " distill and bce loss: 0.9174011195147479\n",
            " distill and bce loss: 0.9153785297164211\n",
            " distill and bce loss: 0.9086201378592739\n",
            " distill and bce loss: 0.9106691137508109\n",
            " distill and bce loss: 0.9085292992768464\n",
            " distill and bce loss: 0.9176170836996149\n",
            " distill and bce loss: 0.9147814920655003\n",
            " distill and bce loss: 0.9038607356724916\n",
            "Accuracy on eval NME: 0.49644444444444447\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256, 0.585, 0.5562857142857143, 0.528875, 0.49644444444444447]\n",
            "21/22 exemplars for class: 63 and 220 total exemplars [32, 4, 69, 36, 7, 5, 75, 61, 62, 63]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " Incremental batch num:  9\n",
            "5000 1000\n",
            "100\n",
            "Seen cls number:  100\n",
            " distill and bce loss: 1.5393209810610171\n",
            " distill and bce loss: 1.5909275478786893\n",
            " distill and bce loss: 1.552907786987446\n",
            " distill and bce loss: 1.5061813482531794\n",
            " distill and bce loss: 1.4540050272588376\n",
            " distill and bce loss: 1.4375014570024278\n",
            " distill and bce loss: 1.4450703020449038\n",
            " distill and bce loss: 1.4376026193300884\n",
            " distill and bce loss: 1.4352048127739518\n",
            " distill and bce loss: 1.4257066360226385\n",
            " distill and bce loss: 1.4077459595821522\n",
            " distill and bce loss: 1.4013080685227006\n",
            " distill and bce loss: 1.4041409823629591\n",
            " distill and bce loss: 1.4023165813198797\n",
            " distill and bce loss: 1.402646091249254\n",
            " distill and bce loss: 1.3889351420932345\n",
            " distill and bce loss: 1.3897916586310775\n",
            " distill and bce loss: 1.3943803266242698\n",
            " distill and bce loss: 1.3924742076132033\n",
            " distill and bce loss: 1.382194713309959\n",
            " distill and bce loss: 1.3824585190525762\n",
            " distill and bce loss: 1.374437431494395\n",
            " distill and bce loss: 1.384257638895953\n",
            " distill and bce loss: 1.3710066345002916\n",
            " distill and bce loss: 1.3742271573455245\n",
            " distill and bce loss: 1.38413597257049\n",
            " distill and bce loss: 1.3785754000699078\n",
            " distill and bce loss: 1.3791402688732854\n",
            " distill and bce loss: 1.3658108954076413\n",
            " distill and bce loss: 1.3684895656726979\n",
            " distill and bce loss: 1.370380492122085\n",
            " distill and bce loss: 1.3747399714257982\n",
            " distill and bce loss: 1.3598360330970198\n",
            " distill and bce loss: 1.3776854541566637\n",
            " distill and bce loss: 1.3681105860957392\n",
            " distill and bce loss: 1.3735384786570515\n",
            " distill and bce loss: 1.363443171536481\n",
            " distill and bce loss: 1.3646012235570837\n",
            " distill and bce loss: 1.382483133563289\n",
            " distill and bce loss: 1.3855170475112066\n",
            " distill and bce loss: 1.300383832719591\n",
            " distill and bce loss: 1.2751826379034255\n",
            " distill and bce loss: 1.2534648422841672\n",
            " distill and bce loss: 1.2532499101426866\n",
            " distill and bce loss: 1.2500723997751872\n",
            " distill and bce loss: 1.2409702362837616\n",
            " distill and bce loss: 1.2370356144728485\n",
            " distill and bce loss: 1.231854189325262\n",
            " distill and bce loss: 1.230511552757687\n",
            " distill and bce loss: 1.2346469737865307\n",
            " distill and bce loss: 1.2191153190754078\n",
            " distill and bce loss: 1.2246556524877195\n",
            " distill and bce loss: 1.2303151709062081\n",
            " distill and bce loss: 1.230390151341756\n",
            " distill and bce loss: 1.2195722902262653\n",
            " distill and bce loss: 1.2209409983069808\n",
            " distill and bce loss: 1.2189346397364582\n",
            " distill and bce loss: 1.2221347402643274\n",
            " distill and bce loss: 1.216370211707221\n",
            " distill and bce loss: 1.221875958972507\n",
            " distill and bce loss: 1.2147773040665522\n",
            " distill and bce loss: 1.2069447526225336\n",
            " distill and bce loss: 1.2115862568219502\n",
            " distill and bce loss: 1.2164316640959845\n",
            " distill and bce loss: 1.196239213148753\n",
            " distill and bce loss: 1.2028716692218073\n",
            " distill and bce loss: 1.2040309530717355\n",
            " distill and bce loss: 1.2092055170624345\n",
            " distill and bce loss: 1.2063586778110928\n",
            " distill and bce loss: 1.2033418240370575\n",
            "Accuracy on eval NME: 0.4671\n",
            "[0.871, 0.7705, 0.723, 0.65925, 0.6256, 0.585, 0.5562857142857143, 0.528875, 0.49644444444444447, 0.4671]\n",
            "19/20 exemplars for class: 29 and 200 total exemplars [33, 67, 39, 73, 48, 49, 29, 51, 57, 93]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}