{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256,
     "referenced_widgets": [
      "92a31e1925484ff1b04e2ec3526ab725",
      "aaca349495384f6d9cc855aa85cce535",
      "add41eb5e5fd40ab8d987907d86447bd",
      "a95993f88dfa4bdd833663da916d985c",
      "dd1ce71ab5b04e99a7fd7802714ce1b0",
      "ee23a487503f436a8b1f5b961f5edeb3",
      "974bde1b347e48cea02746332bed5fbd",
      "97225e5831714e5fb6ef6fa44581e53f"
     ]
    },
    "colab_type": "code",
    "id": "qMFTx_iVpWrq",
    "outputId": "ea9e5d05-ff49-4f54-a770-5edfc52cbd83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68 56 78  8 23 84 90 65 74 76 40 89  3 92 55  9 26 80 43 38 58 70 77  1\n",
      " 85 19 17 50 28 53 13 81 45 82  6 59 83 16 15 44 91 41 72 60 79 52 20 10\n",
      " 31 54 37 95 14 71 96 98 97  2 64 66 42 22 35 86 24 34 87 21 99  0 88 27\n",
      " 18 94 11 12 47 25 30 46 62 69 36 61  7 63 75  5 32  4 51 48 73 93 39 67\n",
      " 29 49 57 33]\n",
      "{68: 0, 56: 1, 78: 2, 8: 3, 23: 4, 84: 5, 90: 6, 65: 7, 74: 8, 76: 9, 40: 10, 89: 11, 3: 12, 92: 13, 55: 14, 9: 15, 26: 16, 80: 17, 43: 18, 38: 19, 58: 20, 70: 21, 77: 22, 1: 23, 85: 24, 19: 25, 17: 26, 50: 27, 28: 28, 53: 29, 13: 30, 81: 31, 45: 32, 82: 33, 6: 34, 59: 35, 83: 36, 16: 37, 15: 38, 44: 39, 91: 40, 41: 41, 72: 42, 60: 43, 79: 44, 52: 45, 20: 46, 10: 47, 31: 48, 54: 49, 37: 50, 95: 51, 14: 52, 71: 53, 96: 54, 98: 55, 97: 56, 2: 57, 64: 58, 66: 59, 42: 60, 22: 61, 35: 62, 86: 63, 24: 64, 34: 65, 87: 66, 21: 67, 99: 68, 0: 69, 88: 70, 27: 71, 18: 72, 94: 73, 11: 74, 12: 75, 47: 76, 25: 77, 30: 78, 46: 79, 62: 80, 69: 81, 36: 82, 61: 83, 7: 84, 63: 85, 75: 86, 5: 87, 32: 88, 4: 89, 51: 90, 48: 91, 73: 92, 93: 93, 39: 94, 67: 95, 29: 96, 49: 97, 57: 98, 33: 99}\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to .data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a31e1925484ff1b04e2ec3526ab725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .data/cifar-100-python.tar.gz to .data/\n",
      "Files already downloaded and verified\n",
      "\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets import VisionDataset, CIFAR100\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import ChainMap\n",
    "\n",
    "\n",
    "np.random.seed(1993) \n",
    "order = np.arange(100)\n",
    "np.random.shuffle(order)\n",
    "print(order)\n",
    "label_map = {k: v for v, k in enumerate(order)}\n",
    "\n",
    "\n",
    "\n",
    "print(label_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = CIFAR100('.data/', train=True, transform=None, download=True)\n",
    "test = CIFAR100('.data/', train=False, transform=None, download=True)\n",
    "\n",
    "class Cifar100:\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.train_groups, self.test_groups = self.split()\n",
    "        self.batch_num = 10\n",
    "\n",
    "    def split(self):\n",
    "        train_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
    "        for data, target in self.train:\n",
    "            if target in order[:10]:\n",
    "                train_groups[0].append((data,target))\n",
    "            elif target in order[10:20]:\n",
    "                train_groups[1].append((data,target))\n",
    "            elif target in order[20:30]:\n",
    "                train_groups[2].append((data,target))\n",
    "            elif target in order[30:40]:\n",
    "                train_groups[3].append((data,target))\n",
    "            elif target in order[40:50]:\n",
    "                train_groups[4].append((data,target))\n",
    "            elif target in order[50:60]:\n",
    "                train_groups[5].append((data,target))\n",
    "            elif target in order[60:70]:\n",
    "                train_groups[6].append((data,target))\n",
    "            elif target in order[70:80]:\n",
    "                train_groups[7].append((data,target))\n",
    "            elif target in order[80:90]:\n",
    "                train_groups[8].append((data,target))\n",
    "            elif target in order[90:100]:\n",
    "                train_groups[9].append((data,target))\n",
    "        assert len(train_groups[0]) == 5000, len(train_groups[0])\n",
    "        assert len(train_groups[1]) == 5000, len(train_groups[1])\n",
    "        assert len(train_groups[2]) == 5000, len(train_groups[2])\n",
    "        assert len(train_groups[3]) == 5000, len(train_groups[3])\n",
    "        assert len(train_groups[4]) == 5000, len(train_groups[4])\n",
    "        assert len(train_groups[5]) == 5000, len(train_groups[5])\n",
    "        assert len(train_groups[6]) == 5000, len(train_groups[6])\n",
    "        assert len(train_groups[7]) == 5000, len(train_groups[7])\n",
    "        assert len(train_groups[8]) == 5000, len(train_groups[8])\n",
    "        assert len(train_groups[9]) == 5000, len(train_groups[9])\n",
    "\n",
    "\n",
    "\n",
    "        test_groups = [[],[],[],[],[],[],[],[],[],[]]\n",
    "        for data, target in self.test:\n",
    "            if target in order[:10]:\n",
    "                test_groups[0].append((data,target))\n",
    "            elif target in order[10:20]:\n",
    "                test_groups[1].append((data,target))\n",
    "            elif target in order[20:30]:\n",
    "                test_groups[2].append((data,target))\n",
    "            elif target in order[30:40]:\n",
    "                test_groups[3].append((data,target))\n",
    "            elif target in order[40:50]:\n",
    "                test_groups[4].append((data,target))\n",
    "            elif target in order[50:60]:\n",
    "                test_groups[5].append((data,target))\n",
    "            elif target in order[60:70]:\n",
    "                test_groups[6].append((data,target))\n",
    "            elif target in order[70:80]:\n",
    "                test_groups[7].append((data,target))\n",
    "            elif target in order[80:90]:\n",
    "                test_groups[8].append((data,target))\n",
    "            elif target in order[90:100]:\n",
    "                test_groups[9].append((data,target))\n",
    "        assert len(test_groups[0]) == 1000\n",
    "        assert len(test_groups[1]) == 1000\n",
    "        assert len(test_groups[2]) == 1000\n",
    "        assert len(test_groups[3]) == 1000\n",
    "        assert len(test_groups[4]) == 1000\n",
    "        assert len(test_groups[5]) == 1000\n",
    "        assert len(test_groups[6]) == 1000\n",
    "        assert len(test_groups[7]) == 1000\n",
    "        assert len(test_groups[8]) == 1000\n",
    "        assert len(test_groups[9]) == 1000\n",
    "\n",
    "        return train_groups, test_groups\n",
    "\n",
    "    def next_classes_batch(self, i):\n",
    "        return self.train_groups[i], self.test_groups[i]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cifar = Cifar100(train, test)\n",
    "    print(len(cifar.train_groups[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwMuXfutph8I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import skimage.io as io\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class BatchData(Dataset):\n",
    "    def __init__(self, images, labels, input_transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        image = Image.fromarray(np.uint8(image))\n",
    "        label = self.labels[index]\n",
    "        label = label_map[label]\n",
    "        if self.input_transform is not None:\n",
    "          image = self.input_transform(image)\n",
    "        \n",
    "        label = torch.LongTensor([label])\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJV7GMX8pkRc"
   },
   "outputs": [],
   "source": [
    "class Exemplar:\n",
    "    def __init__(self, max_size, total_cls):    # serve total_cls?\n",
    "        self.train = {}\n",
    "        self.cur_cls = 0\n",
    "        self.max_size = max_size\n",
    "        self.total_classes = total_cls\n",
    "        self.store_num = {}\n",
    "        self.count_train = {}\n",
    "\n",
    "    def update(self, cls_num, train, inc_i):\n",
    "        train_x, train_y = train\n",
    "        cur_keys = list(set(train_y))\n",
    "        print(cur_keys)\n",
    "        def countX(tup, x): \n",
    "            count = 0\n",
    "            for ele in tup: \n",
    "              if (ele == x): \n",
    "                count = count + 1\n",
    "            return count \n",
    "        \n",
    "        self.cur_cls = cls_num\n",
    "\n",
    "        for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
    "            self.count_train[i] = countX(train_y, i)     \n",
    "            self.store_num[i] = int(self.count_train[i] / ((self.cur_cls)*0.1))\n",
    "            # print(i, self.store_num[i])    # to balance val new and val old\n",
    "\n",
    "\n",
    "        #assert self.cur_cls == len(list(self.train.keys()))\n",
    "\n",
    "        \n",
    "        total_store_num = self.max_size / self.cur_cls #if self.cur_cls != 0 else max_size\n",
    "        train_store_num = round(total_store_num)  # ha senso?\n",
    "\n",
    "\n",
    "        for x, y in zip(train_x, train_y):\n",
    "            if y not in self.train:\n",
    "                self.train[y] = [x]\n",
    "            else:\n",
    "                #if len(self.train[y]) < self.store_num[y]:\n",
    "                if len(self.train[y]) < train_store_num:\n",
    "                    self.train[y].append(x)\n",
    "        assert self.cur_cls == len(list(self.train.keys()))\n",
    "        \n",
    "        for key, value in self.train.items():\n",
    "            #self.store_num[key] = int(self.count_train[key] / ((self.cur_cls)*0.1))\n",
    "            #self.train[key] = value[:self.store_num[key]]\n",
    "            self.train[key] = value[:train_store_num]\n",
    "        for key, value in self.train.items():\n",
    "            #assert len(self.train[key]) == self.store_num[key]\n",
    "            assert len(self.train[key]) == train_store_num      # ==!\n",
    "\n",
    "\n",
    "    def get_exemplar_train(self):\n",
    "        exemplar_train_x = []\n",
    "        exemplar_train_y = []\n",
    "        for key, value in self.train.items():\n",
    "            for train_x in value:\n",
    "                exemplar_train_x.append(train_x)\n",
    "                exemplar_train_y.append(key)\n",
    "        return exemplar_train_x, exemplar_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSWE63gUpoKs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jASHQHW4pqCN"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\"\"\"\n",
    "Credits to @hshustc\n",
    "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        self.old_model = None\n",
    "        self.num_classes = num_classes\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, self.num_classes)\n",
    "  \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, features=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if features:\n",
    "            x = x / x.norm()\n",
    "        else:\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "def resnet20(pretrained=False, **kwargs):\n",
    "    n = 3\n",
    "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet32(pretrained=False, **kwargs):\n",
    "    n = 5\n",
    "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet56(pretrained=False, **kwargs):\n",
    "    n = 9\n",
    "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMOW7LKTptRp"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xy3xT3ftpz4w"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, Scale, Resize, ToTensor, ToPILImage\n",
    "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss    \n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "from matplotlib.pyplot import nipy_spectral\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import manifold, datasets\n",
    "from time import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import PIL.Image as Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "# import dml\n",
    "# from dml.ncmc import NCMC_Classifier\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, total_cls):\n",
    "        self.total_cls = total_cls\n",
    "        self.seen_cls = 0\n",
    "        self.dataset = Cifar100(train, test)\n",
    "        self.model = resnet32(num_classes = total_cls).cuda()\n",
    "        self.count_label = {}\n",
    "        samples_per_cls = [[0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100, [0] * 100]\n",
    "        for inc_i in range(10):\n",
    "            assert len(samples_per_cls[inc_i]) == 100\n",
    "            for j in range(10):\n",
    "                samples_per_cls[inc_i][10*inc_i+j] = 500\n",
    "            for i in range(inc_i*10):\n",
    "                samples_per_cls[inc_i][i]=int(200/(inc_i))\n",
    "        beta = 0.999\n",
    "        effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "        weights = (1.0 - beta) / np.array(effective_num)\n",
    "        # weights = weights / np.sum(weights) * 100\n",
    "        self.weights = torch.Tensor(weights).cuda()\n",
    "        self.means = {}\n",
    "        transform_train = [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ]\n",
    "        transform_train.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        self.input_transform = Compose(transform_train)\n",
    "        \n",
    "        self.input_transform_eval = Compose([\n",
    "                                ToTensor(),\n",
    "                                Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010))])\n",
    "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(\"Solver total trainable parameters : \", total_params)\n",
    "\n",
    "\n",
    "    def test(self, testdata):\n",
    "        # print(\"test data number : \",len(testdata))\n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        with torch.no_grad():\n",
    "          for i, (image, label) in enumerate(testdata):\n",
    "              image = image.cuda()\n",
    "              label = label.view(-1).cuda()\n",
    "              p = self.model(image)\n",
    "              pred = p[:,:self.seen_cls].argmax(dim=-1)\n",
    "              correct += sum(pred == label).item()\n",
    "              wrong += sum(pred != label).item()\n",
    "        acc = correct / (wrong + correct)\n",
    "        print(f\"\\r Test Acc: {acc*100} \\n\")\n",
    "        return acc\n",
    "\n",
    "    def NMEClassifier(self, test_dataloader, train_loader, display=True, suffix=''):\n",
    "      self.model.eval()\n",
    "      with torch.no_grad():\n",
    "        for i in range(0,self.seen_cls):\n",
    "          t=0\n",
    "          mean = torch.zeros((1,64),device='cuda')\n",
    "          for indices,(images,labels) in enumerate((train_loader)):\n",
    "            images = images.to('cuda')\n",
    "            outputs = self.model(images,features=True)\n",
    "            for output,key in zip(outputs,labels):\n",
    "              if i==key:\n",
    "                mean+=output\n",
    "                t+=1\n",
    "          mean = mean/t\n",
    "          self.means[i] = mean / mean.norm()\n",
    "        running_corrects = 0\n",
    "        for images,labels in test_dataloader:\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            features = self.model(images,features=True)\n",
    "\n",
    "            for i,sample in enumerate(features):\n",
    "                dots = torch.tensor([torch.dot(torch.squeeze(mean), sample).data for _,mean in self.means.items()])\n",
    "                y_pred = torch.argmax(dots).item()\n",
    "                if y_pred == labels[i] : \n",
    "                    running_corrects+=1\n",
    "\n",
    "        accuracy_eval = running_corrects / (100*self.seen_cls)\n",
    "\n",
    "        if display :    \n",
    "            print('Accuracy on eval NME'+str(suffix)+':', accuracy_eval)\n",
    "\n",
    "        return accuracy_eval\n",
    "\n",
    "\n",
    "    def get_lr(self, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def train(self, batch_size, epoches, lr, max_size):\n",
    "        total_cls = self.total_cls\n",
    "        criterion = CrossEntropyLoss()\n",
    "        # criterion = BCEWithLogitsLoss()\n",
    "\n",
    "        previous_model = None\n",
    "        exemplar = Exemplar(max_size, total_cls)\n",
    "        dataset = self.dataset\n",
    "        test_xs = []\n",
    "        test_ys = []\n",
    "        train_xs = []\n",
    "        train_ys = []\n",
    "\n",
    "        test_accs = []\n",
    "        for inc_i in range(dataset.batch_num):\n",
    "            print(50*'---')\n",
    "            print(\" Incremental batch num: \" , inc_i)\n",
    "            train, test = dataset.next_classes_batch(inc_i)\n",
    "            print(len(train), len(test))\n",
    "            train_x, train_y = zip(*train)\n",
    "            test_x, test_y = zip(*test)\n",
    "            test_xs.extend(test_x)\n",
    "            test_ys.extend(test_y)\n",
    " \n",
    "\n",
    "            if inc_i > 0:\n",
    "               old_task = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform), batch_size=128, shuffle=True, drop_last=True)\n",
    "               train_xs, train_ys = exemplar.get_exemplar_train()\n",
    "\n",
    "            \n",
    "            train_xs.extend(train_x)\n",
    "            train_ys.extend(train_y)  \n",
    "\n",
    "            def countX(tup, x): \n",
    "              count = 0\n",
    "              for ele in tup: \n",
    "                if (ele == x): \n",
    "                  count = count + 1\n",
    "              return count \n",
    "        \n",
    "\n",
    "            for i in order[inc_i*10: 10*(inc_i+1)]:      # cambia range ad ogni step\n",
    "              self.count_label[i] = countX(train_ys, i)     \n",
    "\n",
    "            print(len(list(self.count_label.keys())))\n",
    "              \n",
    "            self.seen_cls = (total_cls//dataset.batch_num)*(inc_i + 1)\n",
    "            print(\"Seen cls number: \", self.seen_cls)                            \n",
    "\n",
    "            assert self.seen_cls == len(list(self.count_label.keys()))    \n",
    "\n",
    "            train_data = DataLoader(BatchData(train_xs, train_ys, input_transform=self.input_transform),\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            test_data = DataLoader(BatchData(test_xs, test_ys, input_transform=self.input_transform_eval),\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9,  weight_decay=2e-4)\n",
    "            # scheduler = LambdaLR(optimizer, lr_lambda=adjust_cifar100)\n",
    "            # scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "            scheduler = MultiStepLR(optimizer, milestones=[40, 60], gamma=0.2)\n",
    "\n",
    "\n",
    "            test_acc = []\n",
    "            \n",
    "\n",
    "            for epoch in range(epoches):\n",
    "                cur_lr = self.get_lr(optimizer)\n",
    "                print(f\"\\r EPOCH:{epoch}, LR:{cur_lr}\", end='')\n",
    "                if inc_i > 0:\n",
    "                   self.stage_distill(train_data, old_task, criterion, optimizer, inc_i)\n",
    "                   scheduler.step()\n",
    "                else:\n",
    "                   self.stage_1(train_data, criterion, optimizer)\n",
    "                   scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "            self.previous_model = deepcopy(self.model)\n",
    "            if inc_i > 0:\n",
    "              acc = self.NMEClassifier(test_data,train_data)\n",
    "            else:\n",
    "              acc = self.test(test_data)\n",
    "            test_acc.append(acc)\n",
    "            test_accs.append(max(test_acc))\n",
    "            print(test_accs)\n",
    "            \n",
    "            x_ex, y_ex = self.exemplar_selection(train_x, train_y, inc_i)   \n",
    "            exemplar.update(self.seen_cls, (x_ex, y_ex), inc_i)\n",
    "\n",
    "\n",
    "         \n",
    "        fig, ax = plt.subplots()\n",
    "        line1 = ax.plot(range(1, len(test_accs)+1), test_accs, label = 'Test accuracy')\n",
    "        ax.set(xlabel='N. Classes')\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def cod_one_hot(self, x):\n",
    "        batch_size = 128\n",
    "        y_one_hot = torch.zeros(len(x), self.seen_cls).cuda()\n",
    "        y_one_hot = y_one_hot.scatter(1,x.long().view(-1,1).cuda(),1).cuda()\n",
    "        return y_one_hot\n",
    "\n",
    "\n",
    "    def stage_1(self, train_data, criterion, optimizer):\n",
    "        self.model.train()\n",
    "        print(f\"\\r Training new classes... \", end='')\n",
    "        losses = []\n",
    "        for i, (image, label) in enumerate((train_data)):\n",
    "            image = image.cuda()\n",
    "            label = label.view(-1).cuda() \n",
    "            # label = self.cod_one_hot(label)\n",
    "            p = self.model(image)\n",
    "            loss = criterion(p[:,:self.seen_cls], label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"\\r stage1 loss: {np.mean(losses)} \\n\", end='')\n",
    "\n",
    "\n",
    "    def exemplar_selection(self,x,y,inc_i):\n",
    "        print(f\"\\r Selecting exemplars... \", end='')\n",
    "\n",
    "        set_class=list(set(y))\n",
    "        m=round(2000/self.seen_cls)\n",
    "        # Initialize list of means, images and exemplars for each class\n",
    "        means = dict.fromkeys(set_class)\n",
    "        class_map = dict.fromkeys(set_class)\n",
    "        exemplars = dict.fromkeys(set_class)\n",
    "\n",
    "        for label in class_map:\n",
    "          class_map[label] =  []\n",
    "          exemplars[label] = []\n",
    "        # Fill class_map\n",
    "        for item,la in zip(x,y):\n",
    "          for label in class_map:\n",
    "            if la == label:\n",
    "              class_map[label].append(item)\n",
    "        # Get and save net outputs for each class\n",
    "        self.model.eval()\n",
    "        X_exemplars=[]\n",
    "        y_exemplars=[]\n",
    "        for label in class_map:\n",
    "          class_outputs = []\n",
    "          tr_y=[label]*len(class_map[label])\n",
    "          # Compute class means\n",
    "          with torch.no_grad():\n",
    "            loader = DataLoader(BatchData(class_map[label],tr_y , input_transform=self.input_transform_eval),batch_size=128, shuffle=False,num_workers=4,drop_last=False)\n",
    "            for images, _ in loader:\n",
    "                images = images.to('cuda')\n",
    "                outputs = self.model(images,features=True)\n",
    "                for output in outputs:\n",
    "                    output = output.to('cuda')\n",
    "                    class_outputs.append(output.data.cpu().numpy())\n",
    "          # Construct exemplar list for current class\n",
    "          features_exemplars = []\n",
    "          features = np.array(class_outputs)\n",
    "          features = F.normalize(torch.tensor(features))\n",
    "          X = features\n",
    "\n",
    "          pca = PCA(n_components=2, random_state=0)\n",
    "          X = pca.fit_transform(X)\n",
    "          n_samples = 500\n",
    "          n_components = 2\n",
    "          (fig, subplots) = plt.subplots(1,4,figsize=(24, 6))\n",
    "          perplexities = [10, 25, 50]\n",
    "          kmeans_pca = KMeans(n_clusters = m, random_state = 0)\n",
    "          kmeans = KMeans(n_clusters = m, random_state = 0)\n",
    "          color = kmeans_pca.fit_predict(X)\n",
    "          kmeans.fit(features)\n",
    "          ax = subplots[0]\n",
    "          ax.scatter(X[:, 0], X[:, 1], c=color, cmap='nipy_spectral')\n",
    "          ax.xaxis.set_major_formatter(NullFormatter())\n",
    "          ax.yaxis.set_major_formatter(NullFormatter())\n",
    " \n",
    "          for i, perplexity in enumerate(perplexities):\n",
    "              ax = subplots[i+1]\n",
    "\n",
    "              tsne = manifold.TSNE(n_components=n_components, init='pca',\n",
    "                         random_state=0, perplexity=perplexity)\n",
    "              Y = tsne.fit_transform(X)\n",
    "              # print(\"S-curve, perplexity=%d \" % (perplexity))\n",
    "\n",
    "              ax.set_title(\"Perplexity=%d\" % perplexity)\n",
    "              ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap='nipy_spectral')\n",
    "              ax.xaxis.set_major_formatter(NullFormatter())\n",
    "              ax.yaxis.set_major_formatter(NullFormatter())\n",
    "              ax.axis('tight')\n",
    "          # plt.show()\n",
    "          for i in range(m):\n",
    "            i_vector = np.argmin( np.sqrt( np.sum( (torch.tensor(kmeans.cluster_centers_[i]).cpu().numpy() - features.cpu().numpy())**2, axis=1 ) ) )\n",
    "\n",
    "            features_exemplars.append(features[i_vector])\n",
    "            X_exemplars.append(class_map[label][i_vector])\n",
    "            y_exemplars.append(label)\n",
    "\n",
    "            print(f'\\r{(i+1)}/{m} exemplars for class: {label} and {len(X_exemplars)} total exemplars ', end='')\n",
    "        return X_exemplars, y_exemplars\n",
    "\n",
    "    def stage_distill(self, train_data, old_task, criterion, optimizer, inc_i):\n",
    "        self.model.train()\n",
    "        print(f\"\\r Training ... \", end='')\n",
    "        distill_losses = []\n",
    "        lambd = (10 * (inc_i)) / 100\n",
    "        for i, (image, label) in enumerate((train_data)):\n",
    "            image = image.cuda()\n",
    "            label = label.view(-1).cuda()\n",
    "            # label = self.cod_one_hot(label)\n",
    "            p = self.model(image)\n",
    "            with torch.no_grad():\n",
    "                pre_p = self.previous_model(image)\n",
    "            pre_p = F.softmax(pre_p[:,:self.seen_cls-10], dim=1)\n",
    "            logp = F.log_softmax(p[:,:self.seen_cls-10], dim=1)\n",
    "            dist_loss = -torch.mean(torch.sum(pre_p * logp, dim=1))\n",
    "            # ewc_loss = 100 * ewc.penalty(self.model)\n",
    "            loss_class = CrossEntropyLoss()(p[:,:self.seen_cls], label)\n",
    "            loss = (1-lambd) * loss_class + lambd * dist_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            distill_losses.append(loss.item())\n",
    "        print(f\"\\r distill and bce loss: {np.mean(distill_losses)}\\n\",end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils import *\n",
    "import argparse\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Incremental Learning BIC')\n",
    "parser.add_argument('--batch_size', default = 128, type = int)\n",
    "parser.add_argument('--epoch', default = 70, type = int)\n",
    "parser.add_argument('--lr', default = 0.1, type = int)\n",
    "parser.add_argument('--max_size', default = 2000, type = int)\n",
    "parser.add_argument('--total_cls', default = 100, type = int)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "trainer = Trainer(args.total_cls)\n",
    "trainer.train(args.batch_size, args.epoch, args.lr, args.max_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ewc_kmeans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "92a31e1925484ff1b04e2ec3526ab725": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_add41eb5e5fd40ab8d987907d86447bd",
       "IPY_MODEL_a95993f88dfa4bdd833663da916d985c"
      ],
      "layout": "IPY_MODEL_aaca349495384f6d9cc855aa85cce535"
     }
    },
    "97225e5831714e5fb6ef6fa44581e53f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "974bde1b347e48cea02746332bed5fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a95993f88dfa4bdd833663da916d985c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97225e5831714e5fb6ef6fa44581e53f",
      "placeholder": "​",
      "style": "IPY_MODEL_974bde1b347e48cea02746332bed5fbd",
      "value": " 169009152/? [00:09&lt;00:00, 17942945.31it/s]"
     }
    },
    "aaca349495384f6d9cc855aa85cce535": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "add41eb5e5fd40ab8d987907d86447bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee23a487503f436a8b1f5b961f5edeb3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dd1ce71ab5b04e99a7fd7802714ce1b0",
      "value": 1
     }
    },
    "dd1ce71ab5b04e99a7fd7802714ce1b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ee23a487503f436a8b1f5b961f5edeb3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
